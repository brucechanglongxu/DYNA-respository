{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c364732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bf01f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, Tuple, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.special import softmax\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    AdaLoraModel\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gpn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a833370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotTokenizer:\n",
    "    def __init__(self, vocab_path):\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocabulary = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        # Here, the tokens are individual characters (like A, C, G, T)\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.vocabulary)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "        self.token_length = 1  # Each nucleotide is considered separately in one-hot encoding\n",
    "\n",
    "    def tokenize(self, sequence):\n",
    "        # In one-hot encoding, each character/nucleotide is a token\n",
    "        tokens = [sequence[i:i+self.token_length] for i in range(len(sequence))]\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.token_to_id.get(token, self.token_to_id.get('[UNK]')) for token in tokens]\n",
    "\n",
    "    def __call__(self, sequence):\n",
    "        tokens = self.tokenize(sequence)\n",
    "        return self.convert_tokens_to_ids(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dac335ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlappingTokenizer:\n",
    "    def __init__(self, vocab_path):\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocabulary = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.vocabulary)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "        self.token_length = 6  # Overlapping length\n",
    "\n",
    "    def tokenize(self, sequence):\n",
    "        tokens = [sequence[i:i+self.token_length] for i in range(0, len(sequence) - self.token_length + 1)]\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.token_to_id.get(token, self.token_to_id.get('[UNK]')) for token in tokens]\n",
    "\n",
    "    def __call__(self, sequence):\n",
    "        tokens = self.tokenize(sequence)\n",
    "        return self.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94100ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '<pad>': 1, '<mask>': 2, '<cls>': 3, 'AAAAAA': 4, 'AAAAAT': 5, 'AAAAAC': 6, 'AAAAAG': 7, 'AAAATA': 8, 'AAAATT': 9, 'AAAATC': 10, 'AAAATG': 11, 'AAAACA': 12, 'AAAACT': 13, 'AAAACC': 14, 'AAAACG': 15, 'AAAAGA': 16, 'AAAAGT': 17, 'AAAAGC': 18, 'AAAAGG': 19, 'AAATAA': 20, 'AAATAT': 21, 'AAATAC': 22, 'AAATAG': 23, 'AAATTA': 24, 'AAATTT': 25, 'AAATTC': 26, 'AAATTG': 27, 'AAATCA': 28, 'AAATCT': 29, 'AAATCC': 30, 'AAATCG': 31, 'AAATGA': 32, 'AAATGT': 33, 'AAATGC': 34, 'AAATGG': 35, 'AAACAA': 36, 'AAACAT': 37, 'AAACAC': 38, 'AAACAG': 39, 'AAACTA': 40, 'AAACTT': 41, 'AAACTC': 42, 'AAACTG': 43, 'AAACCA': 44, 'AAACCT': 45, 'AAACCC': 46, 'AAACCG': 47, 'AAACGA': 48, 'AAACGT': 49, 'AAACGC': 50, 'AAACGG': 51, 'AAAGAA': 52, 'AAAGAT': 53, 'AAAGAC': 54, 'AAAGAG': 55, 'AAAGTA': 56, 'AAAGTT': 57, 'AAAGTC': 58, 'AAAGTG': 59, 'AAAGCA': 60, 'AAAGCT': 61, 'AAAGCC': 62, 'AAAGCG': 63, 'AAAGGA': 64, 'AAAGGT': 65, 'AAAGGC': 66, 'AAAGGG': 67, 'AATAAA': 68, 'AATAAT': 69, 'AATAAC': 70, 'AATAAG': 71, 'AATATA': 72, 'AATATT': 73, 'AATATC': 74, 'AATATG': 75, 'AATACA': 76, 'AATACT': 77, 'AATACC': 78, 'AATACG': 79, 'AATAGA': 80, 'AATAGT': 81, 'AATAGC': 82, 'AATAGG': 83, 'AATTAA': 84, 'AATTAT': 85, 'AATTAC': 86, 'AATTAG': 87, 'AATTTA': 88, 'AATTTT': 89, 'AATTTC': 90, 'AATTTG': 91, 'AATTCA': 92, 'AATTCT': 93, 'AATTCC': 94, 'AATTCG': 95, 'AATTGA': 96, 'AATTGT': 97, 'AATTGC': 98, 'AATTGG': 99, 'AATCAA': 100, 'AATCAT': 101, 'AATCAC': 102, 'AATCAG': 103, 'AATCTA': 104, 'AATCTT': 105, 'AATCTC': 106, 'AATCTG': 107, 'AATCCA': 108, 'AATCCT': 109, 'AATCCC': 110, 'AATCCG': 111, 'AATCGA': 112, 'AATCGT': 113, 'AATCGC': 114, 'AATCGG': 115, 'AATGAA': 116, 'AATGAT': 117, 'AATGAC': 118, 'AATGAG': 119, 'AATGTA': 120, 'AATGTT': 121, 'AATGTC': 122, 'AATGTG': 123, 'AATGCA': 124, 'AATGCT': 125, 'AATGCC': 126, 'AATGCG': 127, 'AATGGA': 128, 'AATGGT': 129, 'AATGGC': 130, 'AATGGG': 131, 'AACAAA': 132, 'AACAAT': 133, 'AACAAC': 134, 'AACAAG': 135, 'AACATA': 136, 'AACATT': 137, 'AACATC': 138, 'AACATG': 139, 'AACACA': 140, 'AACACT': 141, 'AACACC': 142, 'AACACG': 143, 'AACAGA': 144, 'AACAGT': 145, 'AACAGC': 146, 'AACAGG': 147, 'AACTAA': 148, 'AACTAT': 149, 'AACTAC': 150, 'AACTAG': 151, 'AACTTA': 152, 'AACTTT': 153, 'AACTTC': 154, 'AACTTG': 155, 'AACTCA': 156, 'AACTCT': 157, 'AACTCC': 158, 'AACTCG': 159, 'AACTGA': 160, 'AACTGT': 161, 'AACTGC': 162, 'AACTGG': 163, 'AACCAA': 164, 'AACCAT': 165, 'AACCAC': 166, 'AACCAG': 167, 'AACCTA': 168, 'AACCTT': 169, 'AACCTC': 170, 'AACCTG': 171, 'AACCCA': 172, 'AACCCT': 173, 'AACCCC': 174, 'AACCCG': 175, 'AACCGA': 176, 'AACCGT': 177, 'AACCGC': 178, 'AACCGG': 179, 'AACGAA': 180, 'AACGAT': 181, 'AACGAC': 182, 'AACGAG': 183, 'AACGTA': 184, 'AACGTT': 185, 'AACGTC': 186, 'AACGTG': 187, 'AACGCA': 188, 'AACGCT': 189, 'AACGCC': 190, 'AACGCG': 191, 'AACGGA': 192, 'AACGGT': 193, 'AACGGC': 194, 'AACGGG': 195, 'AAGAAA': 196, 'AAGAAT': 197, 'AAGAAC': 198, 'AAGAAG': 199, 'AAGATA': 200, 'AAGATT': 201, 'AAGATC': 202, 'AAGATG': 203, 'AAGACA': 204, 'AAGACT': 205, 'AAGACC': 206, 'AAGACG': 207, 'AAGAGA': 208, 'AAGAGT': 209, 'AAGAGC': 210, 'AAGAGG': 211, 'AAGTAA': 212, 'AAGTAT': 213, 'AAGTAC': 214, 'AAGTAG': 215, 'AAGTTA': 216, 'AAGTTT': 217, 'AAGTTC': 218, 'AAGTTG': 219, 'AAGTCA': 220, 'AAGTCT': 221, 'AAGTCC': 222, 'AAGTCG': 223, 'AAGTGA': 224, 'AAGTGT': 225, 'AAGTGC': 226, 'AAGTGG': 227, 'AAGCAA': 228, 'AAGCAT': 229, 'AAGCAC': 230, 'AAGCAG': 231, 'AAGCTA': 232, 'AAGCTT': 233, 'AAGCTC': 234, 'AAGCTG': 235, 'AAGCCA': 236, 'AAGCCT': 237, 'AAGCCC': 238, 'AAGCCG': 239, 'AAGCGA': 240, 'AAGCGT': 241, 'AAGCGC': 242, 'AAGCGG': 243, 'AAGGAA': 244, 'AAGGAT': 245, 'AAGGAC': 246, 'AAGGAG': 247, 'AAGGTA': 248, 'AAGGTT': 249, 'AAGGTC': 250, 'AAGGTG': 251, 'AAGGCA': 252, 'AAGGCT': 253, 'AAGGCC': 254, 'AAGGCG': 255, 'AAGGGA': 256, 'AAGGGT': 257, 'AAGGGC': 258, 'AAGGGG': 259, 'ATAAAA': 260, 'ATAAAT': 261, 'ATAAAC': 262, 'ATAAAG': 263, 'ATAATA': 264, 'ATAATT': 265, 'ATAATC': 266, 'ATAATG': 267, 'ATAACA': 268, 'ATAACT': 269, 'ATAACC': 270, 'ATAACG': 271, 'ATAAGA': 272, 'ATAAGT': 273, 'ATAAGC': 274, 'ATAAGG': 275, 'ATATAA': 276, 'ATATAT': 277, 'ATATAC': 278, 'ATATAG': 279, 'ATATTA': 280, 'ATATTT': 281, 'ATATTC': 282, 'ATATTG': 283, 'ATATCA': 284, 'ATATCT': 285, 'ATATCC': 286, 'ATATCG': 287, 'ATATGA': 288, 'ATATGT': 289, 'ATATGC': 290, 'ATATGG': 291, 'ATACAA': 292, 'ATACAT': 293, 'ATACAC': 294, 'ATACAG': 295, 'ATACTA': 296, 'ATACTT': 297, 'ATACTC': 298, 'ATACTG': 299, 'ATACCA': 300, 'ATACCT': 301, 'ATACCC': 302, 'ATACCG': 303, 'ATACGA': 304, 'ATACGT': 305, 'ATACGC': 306, 'ATACGG': 307, 'ATAGAA': 308, 'ATAGAT': 309, 'ATAGAC': 310, 'ATAGAG': 311, 'ATAGTA': 312, 'ATAGTT': 313, 'ATAGTC': 314, 'ATAGTG': 315, 'ATAGCA': 316, 'ATAGCT': 317, 'ATAGCC': 318, 'ATAGCG': 319, 'ATAGGA': 320, 'ATAGGT': 321, 'ATAGGC': 322, 'ATAGGG': 323, 'ATTAAA': 324, 'ATTAAT': 325, 'ATTAAC': 326, 'ATTAAG': 327, 'ATTATA': 328, 'ATTATT': 329, 'ATTATC': 330, 'ATTATG': 331, 'ATTACA': 332, 'ATTACT': 333, 'ATTACC': 334, 'ATTACG': 335, 'ATTAGA': 336, 'ATTAGT': 337, 'ATTAGC': 338, 'ATTAGG': 339, 'ATTTAA': 340, 'ATTTAT': 341, 'ATTTAC': 342, 'ATTTAG': 343, 'ATTTTA': 344, 'ATTTTT': 345, 'ATTTTC': 346, 'ATTTTG': 347, 'ATTTCA': 348, 'ATTTCT': 349, 'ATTTCC': 350, 'ATTTCG': 351, 'ATTTGA': 352, 'ATTTGT': 353, 'ATTTGC': 354, 'ATTTGG': 355, 'ATTCAA': 356, 'ATTCAT': 357, 'ATTCAC': 358, 'ATTCAG': 359, 'ATTCTA': 360, 'ATTCTT': 361, 'ATTCTC': 362, 'ATTCTG': 363, 'ATTCCA': 364, 'ATTCCT': 365, 'ATTCCC': 366, 'ATTCCG': 367, 'ATTCGA': 368, 'ATTCGT': 369, 'ATTCGC': 370, 'ATTCGG': 371, 'ATTGAA': 372, 'ATTGAT': 373, 'ATTGAC': 374, 'ATTGAG': 375, 'ATTGTA': 376, 'ATTGTT': 377, 'ATTGTC': 378, 'ATTGTG': 379, 'ATTGCA': 380, 'ATTGCT': 381, 'ATTGCC': 382, 'ATTGCG': 383, 'ATTGGA': 384, 'ATTGGT': 385, 'ATTGGC': 386, 'ATTGGG': 387, 'ATCAAA': 388, 'ATCAAT': 389, 'ATCAAC': 390, 'ATCAAG': 391, 'ATCATA': 392, 'ATCATT': 393, 'ATCATC': 394, 'ATCATG': 395, 'ATCACA': 396, 'ATCACT': 397, 'ATCACC': 398, 'ATCACG': 399, 'ATCAGA': 400, 'ATCAGT': 401, 'ATCAGC': 402, 'ATCAGG': 403, 'ATCTAA': 404, 'ATCTAT': 405, 'ATCTAC': 406, 'ATCTAG': 407, 'ATCTTA': 408, 'ATCTTT': 409, 'ATCTTC': 410, 'ATCTTG': 411, 'ATCTCA': 412, 'ATCTCT': 413, 'ATCTCC': 414, 'ATCTCG': 415, 'ATCTGA': 416, 'ATCTGT': 417, 'ATCTGC': 418, 'ATCTGG': 419, 'ATCCAA': 420, 'ATCCAT': 421, 'ATCCAC': 422, 'ATCCAG': 423, 'ATCCTA': 424, 'ATCCTT': 425, 'ATCCTC': 426, 'ATCCTG': 427, 'ATCCCA': 428, 'ATCCCT': 429, 'ATCCCC': 430, 'ATCCCG': 431, 'ATCCGA': 432, 'ATCCGT': 433, 'ATCCGC': 434, 'ATCCGG': 435, 'ATCGAA': 436, 'ATCGAT': 437, 'ATCGAC': 438, 'ATCGAG': 439, 'ATCGTA': 440, 'ATCGTT': 441, 'ATCGTC': 442, 'ATCGTG': 443, 'ATCGCA': 444, 'ATCGCT': 445, 'ATCGCC': 446, 'ATCGCG': 447, 'ATCGGA': 448, 'ATCGGT': 449, 'ATCGGC': 450, 'ATCGGG': 451, 'ATGAAA': 452, 'ATGAAT': 453, 'ATGAAC': 454, 'ATGAAG': 455, 'ATGATA': 456, 'ATGATT': 457, 'ATGATC': 458, 'ATGATG': 459, 'ATGACA': 460, 'ATGACT': 461, 'ATGACC': 462, 'ATGACG': 463, 'ATGAGA': 464, 'ATGAGT': 465, 'ATGAGC': 466, 'ATGAGG': 467, 'ATGTAA': 468, 'ATGTAT': 469, 'ATGTAC': 470, 'ATGTAG': 471, 'ATGTTA': 472, 'ATGTTT': 473, 'ATGTTC': 474, 'ATGTTG': 475, 'ATGTCA': 476, 'ATGTCT': 477, 'ATGTCC': 478, 'ATGTCG': 479, 'ATGTGA': 480, 'ATGTGT': 481, 'ATGTGC': 482, 'ATGTGG': 483, 'ATGCAA': 484, 'ATGCAT': 485, 'ATGCAC': 486, 'ATGCAG': 487, 'ATGCTA': 488, 'ATGCTT': 489, 'ATGCTC': 490, 'ATGCTG': 491, 'ATGCCA': 492, 'ATGCCT': 493, 'ATGCCC': 494, 'ATGCCG': 495, 'ATGCGA': 496, 'ATGCGT': 497, 'ATGCGC': 498, 'ATGCGG': 499, 'ATGGAA': 500, 'ATGGAT': 501, 'ATGGAC': 502, 'ATGGAG': 503, 'ATGGTA': 504, 'ATGGTT': 505, 'ATGGTC': 506, 'ATGGTG': 507, 'ATGGCA': 508, 'ATGGCT': 509, 'ATGGCC': 510, 'ATGGCG': 511, 'ATGGGA': 512, 'ATGGGT': 513, 'ATGGGC': 514, 'ATGGGG': 515, 'ACAAAA': 516, 'ACAAAT': 517, 'ACAAAC': 518, 'ACAAAG': 519, 'ACAATA': 520, 'ACAATT': 521, 'ACAATC': 522, 'ACAATG': 523, 'ACAACA': 524, 'ACAACT': 525, 'ACAACC': 526, 'ACAACG': 527, 'ACAAGA': 528, 'ACAAGT': 529, 'ACAAGC': 530, 'ACAAGG': 531, 'ACATAA': 532, 'ACATAT': 533, 'ACATAC': 534, 'ACATAG': 535, 'ACATTA': 536, 'ACATTT': 537, 'ACATTC': 538, 'ACATTG': 539, 'ACATCA': 540, 'ACATCT': 541, 'ACATCC': 542, 'ACATCG': 543, 'ACATGA': 544, 'ACATGT': 545, 'ACATGC': 546, 'ACATGG': 547, 'ACACAA': 548, 'ACACAT': 549, 'ACACAC': 550, 'ACACAG': 551, 'ACACTA': 552, 'ACACTT': 553, 'ACACTC': 554, 'ACACTG': 555, 'ACACCA': 556, 'ACACCT': 557, 'ACACCC': 558, 'ACACCG': 559, 'ACACGA': 560, 'ACACGT': 561, 'ACACGC': 562, 'ACACGG': 563, 'ACAGAA': 564, 'ACAGAT': 565, 'ACAGAC': 566, 'ACAGAG': 567, 'ACAGTA': 568, 'ACAGTT': 569, 'ACAGTC': 570, 'ACAGTG': 571, 'ACAGCA': 572, 'ACAGCT': 573, 'ACAGCC': 574, 'ACAGCG': 575, 'ACAGGA': 576, 'ACAGGT': 577, 'ACAGGC': 578, 'ACAGGG': 579, 'ACTAAA': 580, 'ACTAAT': 581, 'ACTAAC': 582, 'ACTAAG': 583, 'ACTATA': 584, 'ACTATT': 585, 'ACTATC': 586, 'ACTATG': 587, 'ACTACA': 588, 'ACTACT': 589, 'ACTACC': 590, 'ACTACG': 591, 'ACTAGA': 592, 'ACTAGT': 593, 'ACTAGC': 594, 'ACTAGG': 595, 'ACTTAA': 596, 'ACTTAT': 597, 'ACTTAC': 598, 'ACTTAG': 599, 'ACTTTA': 600, 'ACTTTT': 601, 'ACTTTC': 602, 'ACTTTG': 603, 'ACTTCA': 604, 'ACTTCT': 605, 'ACTTCC': 606, 'ACTTCG': 607, 'ACTTGA': 608, 'ACTTGT': 609, 'ACTTGC': 610, 'ACTTGG': 611, 'ACTCAA': 612, 'ACTCAT': 613, 'ACTCAC': 614, 'ACTCAG': 615, 'ACTCTA': 616, 'ACTCTT': 617, 'ACTCTC': 618, 'ACTCTG': 619, 'ACTCCA': 620, 'ACTCCT': 621, 'ACTCCC': 622, 'ACTCCG': 623, 'ACTCGA': 624, 'ACTCGT': 625, 'ACTCGC': 626, 'ACTCGG': 627, 'ACTGAA': 628, 'ACTGAT': 629, 'ACTGAC': 630, 'ACTGAG': 631, 'ACTGTA': 632, 'ACTGTT': 633, 'ACTGTC': 634, 'ACTGTG': 635, 'ACTGCA': 636, 'ACTGCT': 637, 'ACTGCC': 638, 'ACTGCG': 639, 'ACTGGA': 640, 'ACTGGT': 641, 'ACTGGC': 642, 'ACTGGG': 643, 'ACCAAA': 644, 'ACCAAT': 645, 'ACCAAC': 646, 'ACCAAG': 647, 'ACCATA': 648, 'ACCATT': 649, 'ACCATC': 650, 'ACCATG': 651, 'ACCACA': 652, 'ACCACT': 653, 'ACCACC': 654, 'ACCACG': 655, 'ACCAGA': 656, 'ACCAGT': 657, 'ACCAGC': 658, 'ACCAGG': 659, 'ACCTAA': 660, 'ACCTAT': 661, 'ACCTAC': 662, 'ACCTAG': 663, 'ACCTTA': 664, 'ACCTTT': 665, 'ACCTTC': 666, 'ACCTTG': 667, 'ACCTCA': 668, 'ACCTCT': 669, 'ACCTCC': 670, 'ACCTCG': 671, 'ACCTGA': 672, 'ACCTGT': 673, 'ACCTGC': 674, 'ACCTGG': 675, 'ACCCAA': 676, 'ACCCAT': 677, 'ACCCAC': 678, 'ACCCAG': 679, 'ACCCTA': 680, 'ACCCTT': 681, 'ACCCTC': 682, 'ACCCTG': 683, 'ACCCCA': 684, 'ACCCCT': 685, 'ACCCCC': 686, 'ACCCCG': 687, 'ACCCGA': 688, 'ACCCGT': 689, 'ACCCGC': 690, 'ACCCGG': 691, 'ACCGAA': 692, 'ACCGAT': 693, 'ACCGAC': 694, 'ACCGAG': 695, 'ACCGTA': 696, 'ACCGTT': 697, 'ACCGTC': 698, 'ACCGTG': 699, 'ACCGCA': 700, 'ACCGCT': 701, 'ACCGCC': 702, 'ACCGCG': 703, 'ACCGGA': 704, 'ACCGGT': 705, 'ACCGGC': 706, 'ACCGGG': 707, 'ACGAAA': 708, 'ACGAAT': 709, 'ACGAAC': 710, 'ACGAAG': 711, 'ACGATA': 712, 'ACGATT': 713, 'ACGATC': 714, 'ACGATG': 715, 'ACGACA': 716, 'ACGACT': 717, 'ACGACC': 718, 'ACGACG': 719, 'ACGAGA': 720, 'ACGAGT': 721, 'ACGAGC': 722, 'ACGAGG': 723, 'ACGTAA': 724, 'ACGTAT': 725, 'ACGTAC': 726, 'ACGTAG': 727, 'ACGTTA': 728, 'ACGTTT': 729, 'ACGTTC': 730, 'ACGTTG': 731, 'ACGTCA': 732, 'ACGTCT': 733, 'ACGTCC': 734, 'ACGTCG': 735, 'ACGTGA': 736, 'ACGTGT': 737, 'ACGTGC': 738, 'ACGTGG': 739, 'ACGCAA': 740, 'ACGCAT': 741, 'ACGCAC': 742, 'ACGCAG': 743, 'ACGCTA': 744, 'ACGCTT': 745, 'ACGCTC': 746, 'ACGCTG': 747, 'ACGCCA': 748, 'ACGCCT': 749, 'ACGCCC': 750, 'ACGCCG': 751, 'ACGCGA': 752, 'ACGCGT': 753, 'ACGCGC': 754, 'ACGCGG': 755, 'ACGGAA': 756, 'ACGGAT': 757, 'ACGGAC': 758, 'ACGGAG': 759, 'ACGGTA': 760, 'ACGGTT': 761, 'ACGGTC': 762, 'ACGGTG': 763, 'ACGGCA': 764, 'ACGGCT': 765, 'ACGGCC': 766, 'ACGGCG': 767, 'ACGGGA': 768, 'ACGGGT': 769, 'ACGGGC': 770, 'ACGGGG': 771, 'AGAAAA': 772, 'AGAAAT': 773, 'AGAAAC': 774, 'AGAAAG': 775, 'AGAATA': 776, 'AGAATT': 777, 'AGAATC': 778, 'AGAATG': 779, 'AGAACA': 780, 'AGAACT': 781, 'AGAACC': 782, 'AGAACG': 783, 'AGAAGA': 784, 'AGAAGT': 785, 'AGAAGC': 786, 'AGAAGG': 787, 'AGATAA': 788, 'AGATAT': 789, 'AGATAC': 790, 'AGATAG': 791, 'AGATTA': 792, 'AGATTT': 793, 'AGATTC': 794, 'AGATTG': 795, 'AGATCA': 796, 'AGATCT': 797, 'AGATCC': 798, 'AGATCG': 799, 'AGATGA': 800, 'AGATGT': 801, 'AGATGC': 802, 'AGATGG': 803, 'AGACAA': 804, 'AGACAT': 805, 'AGACAC': 806, 'AGACAG': 807, 'AGACTA': 808, 'AGACTT': 809, 'AGACTC': 810, 'AGACTG': 811, 'AGACCA': 812, 'AGACCT': 813, 'AGACCC': 814, 'AGACCG': 815, 'AGACGA': 816, 'AGACGT': 817, 'AGACGC': 818, 'AGACGG': 819, 'AGAGAA': 820, 'AGAGAT': 821, 'AGAGAC': 822, 'AGAGAG': 823, 'AGAGTA': 824, 'AGAGTT': 825, 'AGAGTC': 826, 'AGAGTG': 827, 'AGAGCA': 828, 'AGAGCT': 829, 'AGAGCC': 830, 'AGAGCG': 831, 'AGAGGA': 832, 'AGAGGT': 833, 'AGAGGC': 834, 'AGAGGG': 835, 'AGTAAA': 836, 'AGTAAT': 837, 'AGTAAC': 838, 'AGTAAG': 839, 'AGTATA': 840, 'AGTATT': 841, 'AGTATC': 842, 'AGTATG': 843, 'AGTACA': 844, 'AGTACT': 845, 'AGTACC': 846, 'AGTACG': 847, 'AGTAGA': 848, 'AGTAGT': 849, 'AGTAGC': 850, 'AGTAGG': 851, 'AGTTAA': 852, 'AGTTAT': 853, 'AGTTAC': 854, 'AGTTAG': 855, 'AGTTTA': 856, 'AGTTTT': 857, 'AGTTTC': 858, 'AGTTTG': 859, 'AGTTCA': 860, 'AGTTCT': 861, 'AGTTCC': 862, 'AGTTCG': 863, 'AGTTGA': 864, 'AGTTGT': 865, 'AGTTGC': 866, 'AGTTGG': 867, 'AGTCAA': 868, 'AGTCAT': 869, 'AGTCAC': 870, 'AGTCAG': 871, 'AGTCTA': 872, 'AGTCTT': 873, 'AGTCTC': 874, 'AGTCTG': 875, 'AGTCCA': 876, 'AGTCCT': 877, 'AGTCCC': 878, 'AGTCCG': 879, 'AGTCGA': 880, 'AGTCGT': 881, 'AGTCGC': 882, 'AGTCGG': 883, 'AGTGAA': 884, 'AGTGAT': 885, 'AGTGAC': 886, 'AGTGAG': 887, 'AGTGTA': 888, 'AGTGTT': 889, 'AGTGTC': 890, 'AGTGTG': 891, 'AGTGCA': 892, 'AGTGCT': 893, 'AGTGCC': 894, 'AGTGCG': 895, 'AGTGGA': 896, 'AGTGGT': 897, 'AGTGGC': 898, 'AGTGGG': 899, 'AGCAAA': 900, 'AGCAAT': 901, 'AGCAAC': 902, 'AGCAAG': 903, 'AGCATA': 904, 'AGCATT': 905, 'AGCATC': 906, 'AGCATG': 907, 'AGCACA': 908, 'AGCACT': 909, 'AGCACC': 910, 'AGCACG': 911, 'AGCAGA': 912, 'AGCAGT': 913, 'AGCAGC': 914, 'AGCAGG': 915, 'AGCTAA': 916, 'AGCTAT': 917, 'AGCTAC': 918, 'AGCTAG': 919, 'AGCTTA': 920, 'AGCTTT': 921, 'AGCTTC': 922, 'AGCTTG': 923, 'AGCTCA': 924, 'AGCTCT': 925, 'AGCTCC': 926, 'AGCTCG': 927, 'AGCTGA': 928, 'AGCTGT': 929, 'AGCTGC': 930, 'AGCTGG': 931, 'AGCCAA': 932, 'AGCCAT': 933, 'AGCCAC': 934, 'AGCCAG': 935, 'AGCCTA': 936, 'AGCCTT': 937, 'AGCCTC': 938, 'AGCCTG': 939, 'AGCCCA': 940, 'AGCCCT': 941, 'AGCCCC': 942, 'AGCCCG': 943, 'AGCCGA': 944, 'AGCCGT': 945, 'AGCCGC': 946, 'AGCCGG': 947, 'AGCGAA': 948, 'AGCGAT': 949, 'AGCGAC': 950, 'AGCGAG': 951, 'AGCGTA': 952, 'AGCGTT': 953, 'AGCGTC': 954, 'AGCGTG': 955, 'AGCGCA': 956, 'AGCGCT': 957, 'AGCGCC': 958, 'AGCGCG': 959, 'AGCGGA': 960, 'AGCGGT': 961, 'AGCGGC': 962, 'AGCGGG': 963, 'AGGAAA': 964, 'AGGAAT': 965, 'AGGAAC': 966, 'AGGAAG': 967, 'AGGATA': 968, 'AGGATT': 969, 'AGGATC': 970, 'AGGATG': 971, 'AGGACA': 972, 'AGGACT': 973, 'AGGACC': 974, 'AGGACG': 975, 'AGGAGA': 976, 'AGGAGT': 977, 'AGGAGC': 978, 'AGGAGG': 979, 'AGGTAA': 980, 'AGGTAT': 981, 'AGGTAC': 982, 'AGGTAG': 983, 'AGGTTA': 984, 'AGGTTT': 985, 'AGGTTC': 986, 'AGGTTG': 987, 'AGGTCA': 988, 'AGGTCT': 989, 'AGGTCC': 990, 'AGGTCG': 991, 'AGGTGA': 992, 'AGGTGT': 993, 'AGGTGC': 994, 'AGGTGG': 995, 'AGGCAA': 996, 'AGGCAT': 997, 'AGGCAC': 998, 'AGGCAG': 999, 'AGGCTA': 1000, 'AGGCTT': 1001, 'AGGCTC': 1002, 'AGGCTG': 1003, 'AGGCCA': 1004, 'AGGCCT': 1005, 'AGGCCC': 1006, 'AGGCCG': 1007, 'AGGCGA': 1008, 'AGGCGT': 1009, 'AGGCGC': 1010, 'AGGCGG': 1011, 'AGGGAA': 1012, 'AGGGAT': 1013, 'AGGGAC': 1014, 'AGGGAG': 1015, 'AGGGTA': 1016, 'AGGGTT': 1017, 'AGGGTC': 1018, 'AGGGTG': 1019, 'AGGGCA': 1020, 'AGGGCT': 1021, 'AGGGCC': 1022, 'AGGGCG': 1023, 'AGGGGA': 1024, 'AGGGGT': 1025, 'AGGGGC': 1026, 'AGGGGG': 1027, 'TAAAAA': 1028, 'TAAAAT': 1029, 'TAAAAC': 1030, 'TAAAAG': 1031, 'TAAATA': 1032, 'TAAATT': 1033, 'TAAATC': 1034, 'TAAATG': 1035, 'TAAACA': 1036, 'TAAACT': 1037, 'TAAACC': 1038, 'TAAACG': 1039, 'TAAAGA': 1040, 'TAAAGT': 1041, 'TAAAGC': 1042, 'TAAAGG': 1043, 'TAATAA': 1044, 'TAATAT': 1045, 'TAATAC': 1046, 'TAATAG': 1047, 'TAATTA': 1048, 'TAATTT': 1049, 'TAATTC': 1050, 'TAATTG': 1051, 'TAATCA': 1052, 'TAATCT': 1053, 'TAATCC': 1054, 'TAATCG': 1055, 'TAATGA': 1056, 'TAATGT': 1057, 'TAATGC': 1058, 'TAATGG': 1059, 'TAACAA': 1060, 'TAACAT': 1061, 'TAACAC': 1062, 'TAACAG': 1063, 'TAACTA': 1064, 'TAACTT': 1065, 'TAACTC': 1066, 'TAACTG': 1067, 'TAACCA': 1068, 'TAACCT': 1069, 'TAACCC': 1070, 'TAACCG': 1071, 'TAACGA': 1072, 'TAACGT': 1073, 'TAACGC': 1074, 'TAACGG': 1075, 'TAAGAA': 1076, 'TAAGAT': 1077, 'TAAGAC': 1078, 'TAAGAG': 1079, 'TAAGTA': 1080, 'TAAGTT': 1081, 'TAAGTC': 1082, 'TAAGTG': 1083, 'TAAGCA': 1084, 'TAAGCT': 1085, 'TAAGCC': 1086, 'TAAGCG': 1087, 'TAAGGA': 1088, 'TAAGGT': 1089, 'TAAGGC': 1090, 'TAAGGG': 1091, 'TATAAA': 1092, 'TATAAT': 1093, 'TATAAC': 1094, 'TATAAG': 1095, 'TATATA': 1096, 'TATATT': 1097, 'TATATC': 1098, 'TATATG': 1099, 'TATACA': 1100, 'TATACT': 1101, 'TATACC': 1102, 'TATACG': 1103, 'TATAGA': 1104, 'TATAGT': 1105, 'TATAGC': 1106, 'TATAGG': 1107, 'TATTAA': 1108, 'TATTAT': 1109, 'TATTAC': 1110, 'TATTAG': 1111, 'TATTTA': 1112, 'TATTTT': 1113, 'TATTTC': 1114, 'TATTTG': 1115, 'TATTCA': 1116, 'TATTCT': 1117, 'TATTCC': 1118, 'TATTCG': 1119, 'TATTGA': 1120, 'TATTGT': 1121, 'TATTGC': 1122, 'TATTGG': 1123, 'TATCAA': 1124, 'TATCAT': 1125, 'TATCAC': 1126, 'TATCAG': 1127, 'TATCTA': 1128, 'TATCTT': 1129, 'TATCTC': 1130, 'TATCTG': 1131, 'TATCCA': 1132, 'TATCCT': 1133, 'TATCCC': 1134, 'TATCCG': 1135, 'TATCGA': 1136, 'TATCGT': 1137, 'TATCGC': 1138, 'TATCGG': 1139, 'TATGAA': 1140, 'TATGAT': 1141, 'TATGAC': 1142, 'TATGAG': 1143, 'TATGTA': 1144, 'TATGTT': 1145, 'TATGTC': 1146, 'TATGTG': 1147, 'TATGCA': 1148, 'TATGCT': 1149, 'TATGCC': 1150, 'TATGCG': 1151, 'TATGGA': 1152, 'TATGGT': 1153, 'TATGGC': 1154, 'TATGGG': 1155, 'TACAAA': 1156, 'TACAAT': 1157, 'TACAAC': 1158, 'TACAAG': 1159, 'TACATA': 1160, 'TACATT': 1161, 'TACATC': 1162, 'TACATG': 1163, 'TACACA': 1164, 'TACACT': 1165, 'TACACC': 1166, 'TACACG': 1167, 'TACAGA': 1168, 'TACAGT': 1169, 'TACAGC': 1170, 'TACAGG': 1171, 'TACTAA': 1172, 'TACTAT': 1173, 'TACTAC': 1174, 'TACTAG': 1175, 'TACTTA': 1176, 'TACTTT': 1177, 'TACTTC': 1178, 'TACTTG': 1179, 'TACTCA': 1180, 'TACTCT': 1181, 'TACTCC': 1182, 'TACTCG': 1183, 'TACTGA': 1184, 'TACTGT': 1185, 'TACTGC': 1186, 'TACTGG': 1187, 'TACCAA': 1188, 'TACCAT': 1189, 'TACCAC': 1190, 'TACCAG': 1191, 'TACCTA': 1192, 'TACCTT': 1193, 'TACCTC': 1194, 'TACCTG': 1195, 'TACCCA': 1196, 'TACCCT': 1197, 'TACCCC': 1198, 'TACCCG': 1199, 'TACCGA': 1200, 'TACCGT': 1201, 'TACCGC': 1202, 'TACCGG': 1203, 'TACGAA': 1204, 'TACGAT': 1205, 'TACGAC': 1206, 'TACGAG': 1207, 'TACGTA': 1208, 'TACGTT': 1209, 'TACGTC': 1210, 'TACGTG': 1211, 'TACGCA': 1212, 'TACGCT': 1213, 'TACGCC': 1214, 'TACGCG': 1215, 'TACGGA': 1216, 'TACGGT': 1217, 'TACGGC': 1218, 'TACGGG': 1219, 'TAGAAA': 1220, 'TAGAAT': 1221, 'TAGAAC': 1222, 'TAGAAG': 1223, 'TAGATA': 1224, 'TAGATT': 1225, 'TAGATC': 1226, 'TAGATG': 1227, 'TAGACA': 1228, 'TAGACT': 1229, 'TAGACC': 1230, 'TAGACG': 1231, 'TAGAGA': 1232, 'TAGAGT': 1233, 'TAGAGC': 1234, 'TAGAGG': 1235, 'TAGTAA': 1236, 'TAGTAT': 1237, 'TAGTAC': 1238, 'TAGTAG': 1239, 'TAGTTA': 1240, 'TAGTTT': 1241, 'TAGTTC': 1242, 'TAGTTG': 1243, 'TAGTCA': 1244, 'TAGTCT': 1245, 'TAGTCC': 1246, 'TAGTCG': 1247, 'TAGTGA': 1248, 'TAGTGT': 1249, 'TAGTGC': 1250, 'TAGTGG': 1251, 'TAGCAA': 1252, 'TAGCAT': 1253, 'TAGCAC': 1254, 'TAGCAG': 1255, 'TAGCTA': 1256, 'TAGCTT': 1257, 'TAGCTC': 1258, 'TAGCTG': 1259, 'TAGCCA': 1260, 'TAGCCT': 1261, 'TAGCCC': 1262, 'TAGCCG': 1263, 'TAGCGA': 1264, 'TAGCGT': 1265, 'TAGCGC': 1266, 'TAGCGG': 1267, 'TAGGAA': 1268, 'TAGGAT': 1269, 'TAGGAC': 1270, 'TAGGAG': 1271, 'TAGGTA': 1272, 'TAGGTT': 1273, 'TAGGTC': 1274, 'TAGGTG': 1275, 'TAGGCA': 1276, 'TAGGCT': 1277, 'TAGGCC': 1278, 'TAGGCG': 1279, 'TAGGGA': 1280, 'TAGGGT': 1281, 'TAGGGC': 1282, 'TAGGGG': 1283, 'TTAAAA': 1284, 'TTAAAT': 1285, 'TTAAAC': 1286, 'TTAAAG': 1287, 'TTAATA': 1288, 'TTAATT': 1289, 'TTAATC': 1290, 'TTAATG': 1291, 'TTAACA': 1292, 'TTAACT': 1293, 'TTAACC': 1294, 'TTAACG': 1295, 'TTAAGA': 1296, 'TTAAGT': 1297, 'TTAAGC': 1298, 'TTAAGG': 1299, 'TTATAA': 1300, 'TTATAT': 1301, 'TTATAC': 1302, 'TTATAG': 1303, 'TTATTA': 1304, 'TTATTT': 1305, 'TTATTC': 1306, 'TTATTG': 1307, 'TTATCA': 1308, 'TTATCT': 1309, 'TTATCC': 1310, 'TTATCG': 1311, 'TTATGA': 1312, 'TTATGT': 1313, 'TTATGC': 1314, 'TTATGG': 1315, 'TTACAA': 1316, 'TTACAT': 1317, 'TTACAC': 1318, 'TTACAG': 1319, 'TTACTA': 1320, 'TTACTT': 1321, 'TTACTC': 1322, 'TTACTG': 1323, 'TTACCA': 1324, 'TTACCT': 1325, 'TTACCC': 1326, 'TTACCG': 1327, 'TTACGA': 1328, 'TTACGT': 1329, 'TTACGC': 1330, 'TTACGG': 1331, 'TTAGAA': 1332, 'TTAGAT': 1333, 'TTAGAC': 1334, 'TTAGAG': 1335, 'TTAGTA': 1336, 'TTAGTT': 1337, 'TTAGTC': 1338, 'TTAGTG': 1339, 'TTAGCA': 1340, 'TTAGCT': 1341, 'TTAGCC': 1342, 'TTAGCG': 1343, 'TTAGGA': 1344, 'TTAGGT': 1345, 'TTAGGC': 1346, 'TTAGGG': 1347, 'TTTAAA': 1348, 'TTTAAT': 1349, 'TTTAAC': 1350, 'TTTAAG': 1351, 'TTTATA': 1352, 'TTTATT': 1353, 'TTTATC': 1354, 'TTTATG': 1355, 'TTTACA': 1356, 'TTTACT': 1357, 'TTTACC': 1358, 'TTTACG': 1359, 'TTTAGA': 1360, 'TTTAGT': 1361, 'TTTAGC': 1362, 'TTTAGG': 1363, 'TTTTAA': 1364, 'TTTTAT': 1365, 'TTTTAC': 1366, 'TTTTAG': 1367, 'TTTTTA': 1368, 'TTTTTT': 1369, 'TTTTTC': 1370, 'TTTTTG': 1371, 'TTTTCA': 1372, 'TTTTCT': 1373, 'TTTTCC': 1374, 'TTTTCG': 1375, 'TTTTGA': 1376, 'TTTTGT': 1377, 'TTTTGC': 1378, 'TTTTGG': 1379, 'TTTCAA': 1380, 'TTTCAT': 1381, 'TTTCAC': 1382, 'TTTCAG': 1383, 'TTTCTA': 1384, 'TTTCTT': 1385, 'TTTCTC': 1386, 'TTTCTG': 1387, 'TTTCCA': 1388, 'TTTCCT': 1389, 'TTTCCC': 1390, 'TTTCCG': 1391, 'TTTCGA': 1392, 'TTTCGT': 1393, 'TTTCGC': 1394, 'TTTCGG': 1395, 'TTTGAA': 1396, 'TTTGAT': 1397, 'TTTGAC': 1398, 'TTTGAG': 1399, 'TTTGTA': 1400, 'TTTGTT': 1401, 'TTTGTC': 1402, 'TTTGTG': 1403, 'TTTGCA': 1404, 'TTTGCT': 1405, 'TTTGCC': 1406, 'TTTGCG': 1407, 'TTTGGA': 1408, 'TTTGGT': 1409, 'TTTGGC': 1410, 'TTTGGG': 1411, 'TTCAAA': 1412, 'TTCAAT': 1413, 'TTCAAC': 1414, 'TTCAAG': 1415, 'TTCATA': 1416, 'TTCATT': 1417, 'TTCATC': 1418, 'TTCATG': 1419, 'TTCACA': 1420, 'TTCACT': 1421, 'TTCACC': 1422, 'TTCACG': 1423, 'TTCAGA': 1424, 'TTCAGT': 1425, 'TTCAGC': 1426, 'TTCAGG': 1427, 'TTCTAA': 1428, 'TTCTAT': 1429, 'TTCTAC': 1430, 'TTCTAG': 1431, 'TTCTTA': 1432, 'TTCTTT': 1433, 'TTCTTC': 1434, 'TTCTTG': 1435, 'TTCTCA': 1436, 'TTCTCT': 1437, 'TTCTCC': 1438, 'TTCTCG': 1439, 'TTCTGA': 1440, 'TTCTGT': 1441, 'TTCTGC': 1442, 'TTCTGG': 1443, 'TTCCAA': 1444, 'TTCCAT': 1445, 'TTCCAC': 1446, 'TTCCAG': 1447, 'TTCCTA': 1448, 'TTCCTT': 1449, 'TTCCTC': 1450, 'TTCCTG': 1451, 'TTCCCA': 1452, 'TTCCCT': 1453, 'TTCCCC': 1454, 'TTCCCG': 1455, 'TTCCGA': 1456, 'TTCCGT': 1457, 'TTCCGC': 1458, 'TTCCGG': 1459, 'TTCGAA': 1460, 'TTCGAT': 1461, 'TTCGAC': 1462, 'TTCGAG': 1463, 'TTCGTA': 1464, 'TTCGTT': 1465, 'TTCGTC': 1466, 'TTCGTG': 1467, 'TTCGCA': 1468, 'TTCGCT': 1469, 'TTCGCC': 1470, 'TTCGCG': 1471, 'TTCGGA': 1472, 'TTCGGT': 1473, 'TTCGGC': 1474, 'TTCGGG': 1475, 'TTGAAA': 1476, 'TTGAAT': 1477, 'TTGAAC': 1478, 'TTGAAG': 1479, 'TTGATA': 1480, 'TTGATT': 1481, 'TTGATC': 1482, 'TTGATG': 1483, 'TTGACA': 1484, 'TTGACT': 1485, 'TTGACC': 1486, 'TTGACG': 1487, 'TTGAGA': 1488, 'TTGAGT': 1489, 'TTGAGC': 1490, 'TTGAGG': 1491, 'TTGTAA': 1492, 'TTGTAT': 1493, 'TTGTAC': 1494, 'TTGTAG': 1495, 'TTGTTA': 1496, 'TTGTTT': 1497, 'TTGTTC': 1498, 'TTGTTG': 1499, 'TTGTCA': 1500, 'TTGTCT': 1501, 'TTGTCC': 1502, 'TTGTCG': 1503, 'TTGTGA': 1504, 'TTGTGT': 1505, 'TTGTGC': 1506, 'TTGTGG': 1507, 'TTGCAA': 1508, 'TTGCAT': 1509, 'TTGCAC': 1510, 'TTGCAG': 1511, 'TTGCTA': 1512, 'TTGCTT': 1513, 'TTGCTC': 1514, 'TTGCTG': 1515, 'TTGCCA': 1516, 'TTGCCT': 1517, 'TTGCCC': 1518, 'TTGCCG': 1519, 'TTGCGA': 1520, 'TTGCGT': 1521, 'TTGCGC': 1522, 'TTGCGG': 1523, 'TTGGAA': 1524, 'TTGGAT': 1525, 'TTGGAC': 1526, 'TTGGAG': 1527, 'TTGGTA': 1528, 'TTGGTT': 1529, 'TTGGTC': 1530, 'TTGGTG': 1531, 'TTGGCA': 1532, 'TTGGCT': 1533, 'TTGGCC': 1534, 'TTGGCG': 1535, 'TTGGGA': 1536, 'TTGGGT': 1537, 'TTGGGC': 1538, 'TTGGGG': 1539, 'TCAAAA': 1540, 'TCAAAT': 1541, 'TCAAAC': 1542, 'TCAAAG': 1543, 'TCAATA': 1544, 'TCAATT': 1545, 'TCAATC': 1546, 'TCAATG': 1547, 'TCAACA': 1548, 'TCAACT': 1549, 'TCAACC': 1550, 'TCAACG': 1551, 'TCAAGA': 1552, 'TCAAGT': 1553, 'TCAAGC': 1554, 'TCAAGG': 1555, 'TCATAA': 1556, 'TCATAT': 1557, 'TCATAC': 1558, 'TCATAG': 1559, 'TCATTA': 1560, 'TCATTT': 1561, 'TCATTC': 1562, 'TCATTG': 1563, 'TCATCA': 1564, 'TCATCT': 1565, 'TCATCC': 1566, 'TCATCG': 1567, 'TCATGA': 1568, 'TCATGT': 1569, 'TCATGC': 1570, 'TCATGG': 1571, 'TCACAA': 1572, 'TCACAT': 1573, 'TCACAC': 1574, 'TCACAG': 1575, 'TCACTA': 1576, 'TCACTT': 1577, 'TCACTC': 1578, 'TCACTG': 1579, 'TCACCA': 1580, 'TCACCT': 1581, 'TCACCC': 1582, 'TCACCG': 1583, 'TCACGA': 1584, 'TCACGT': 1585, 'TCACGC': 1586, 'TCACGG': 1587, 'TCAGAA': 1588, 'TCAGAT': 1589, 'TCAGAC': 1590, 'TCAGAG': 1591, 'TCAGTA': 1592, 'TCAGTT': 1593, 'TCAGTC': 1594, 'TCAGTG': 1595, 'TCAGCA': 1596, 'TCAGCT': 1597, 'TCAGCC': 1598, 'TCAGCG': 1599, 'TCAGGA': 1600, 'TCAGGT': 1601, 'TCAGGC': 1602, 'TCAGGG': 1603, 'TCTAAA': 1604, 'TCTAAT': 1605, 'TCTAAC': 1606, 'TCTAAG': 1607, 'TCTATA': 1608, 'TCTATT': 1609, 'TCTATC': 1610, 'TCTATG': 1611, 'TCTACA': 1612, 'TCTACT': 1613, 'TCTACC': 1614, 'TCTACG': 1615, 'TCTAGA': 1616, 'TCTAGT': 1617, 'TCTAGC': 1618, 'TCTAGG': 1619, 'TCTTAA': 1620, 'TCTTAT': 1621, 'TCTTAC': 1622, 'TCTTAG': 1623, 'TCTTTA': 1624, 'TCTTTT': 1625, 'TCTTTC': 1626, 'TCTTTG': 1627, 'TCTTCA': 1628, 'TCTTCT': 1629, 'TCTTCC': 1630, 'TCTTCG': 1631, 'TCTTGA': 1632, 'TCTTGT': 1633, 'TCTTGC': 1634, 'TCTTGG': 1635, 'TCTCAA': 1636, 'TCTCAT': 1637, 'TCTCAC': 1638, 'TCTCAG': 1639, 'TCTCTA': 1640, 'TCTCTT': 1641, 'TCTCTC': 1642, 'TCTCTG': 1643, 'TCTCCA': 1644, 'TCTCCT': 1645, 'TCTCCC': 1646, 'TCTCCG': 1647, 'TCTCGA': 1648, 'TCTCGT': 1649, 'TCTCGC': 1650, 'TCTCGG': 1651, 'TCTGAA': 1652, 'TCTGAT': 1653, 'TCTGAC': 1654, 'TCTGAG': 1655, 'TCTGTA': 1656, 'TCTGTT': 1657, 'TCTGTC': 1658, 'TCTGTG': 1659, 'TCTGCA': 1660, 'TCTGCT': 1661, 'TCTGCC': 1662, 'TCTGCG': 1663, 'TCTGGA': 1664, 'TCTGGT': 1665, 'TCTGGC': 1666, 'TCTGGG': 1667, 'TCCAAA': 1668, 'TCCAAT': 1669, 'TCCAAC': 1670, 'TCCAAG': 1671, 'TCCATA': 1672, 'TCCATT': 1673, 'TCCATC': 1674, 'TCCATG': 1675, 'TCCACA': 1676, 'TCCACT': 1677, 'TCCACC': 1678, 'TCCACG': 1679, 'TCCAGA': 1680, 'TCCAGT': 1681, 'TCCAGC': 1682, 'TCCAGG': 1683, 'TCCTAA': 1684, 'TCCTAT': 1685, 'TCCTAC': 1686, 'TCCTAG': 1687, 'TCCTTA': 1688, 'TCCTTT': 1689, 'TCCTTC': 1690, 'TCCTTG': 1691, 'TCCTCA': 1692, 'TCCTCT': 1693, 'TCCTCC': 1694, 'TCCTCG': 1695, 'TCCTGA': 1696, 'TCCTGT': 1697, 'TCCTGC': 1698, 'TCCTGG': 1699, 'TCCCAA': 1700, 'TCCCAT': 1701, 'TCCCAC': 1702, 'TCCCAG': 1703, 'TCCCTA': 1704, 'TCCCTT': 1705, 'TCCCTC': 1706, 'TCCCTG': 1707, 'TCCCCA': 1708, 'TCCCCT': 1709, 'TCCCCC': 1710, 'TCCCCG': 1711, 'TCCCGA': 1712, 'TCCCGT': 1713, 'TCCCGC': 1714, 'TCCCGG': 1715, 'TCCGAA': 1716, 'TCCGAT': 1717, 'TCCGAC': 1718, 'TCCGAG': 1719, 'TCCGTA': 1720, 'TCCGTT': 1721, 'TCCGTC': 1722, 'TCCGTG': 1723, 'TCCGCA': 1724, 'TCCGCT': 1725, 'TCCGCC': 1726, 'TCCGCG': 1727, 'TCCGGA': 1728, 'TCCGGT': 1729, 'TCCGGC': 1730, 'TCCGGG': 1731, 'TCGAAA': 1732, 'TCGAAT': 1733, 'TCGAAC': 1734, 'TCGAAG': 1735, 'TCGATA': 1736, 'TCGATT': 1737, 'TCGATC': 1738, 'TCGATG': 1739, 'TCGACA': 1740, 'TCGACT': 1741, 'TCGACC': 1742, 'TCGACG': 1743, 'TCGAGA': 1744, 'TCGAGT': 1745, 'TCGAGC': 1746, 'TCGAGG': 1747, 'TCGTAA': 1748, 'TCGTAT': 1749, 'TCGTAC': 1750, 'TCGTAG': 1751, 'TCGTTA': 1752, 'TCGTTT': 1753, 'TCGTTC': 1754, 'TCGTTG': 1755, 'TCGTCA': 1756, 'TCGTCT': 1757, 'TCGTCC': 1758, 'TCGTCG': 1759, 'TCGTGA': 1760, 'TCGTGT': 1761, 'TCGTGC': 1762, 'TCGTGG': 1763, 'TCGCAA': 1764, 'TCGCAT': 1765, 'TCGCAC': 1766, 'TCGCAG': 1767, 'TCGCTA': 1768, 'TCGCTT': 1769, 'TCGCTC': 1770, 'TCGCTG': 1771, 'TCGCCA': 1772, 'TCGCCT': 1773, 'TCGCCC': 1774, 'TCGCCG': 1775, 'TCGCGA': 1776, 'TCGCGT': 1777, 'TCGCGC': 1778, 'TCGCGG': 1779, 'TCGGAA': 1780, 'TCGGAT': 1781, 'TCGGAC': 1782, 'TCGGAG': 1783, 'TCGGTA': 1784, 'TCGGTT': 1785, 'TCGGTC': 1786, 'TCGGTG': 1787, 'TCGGCA': 1788, 'TCGGCT': 1789, 'TCGGCC': 1790, 'TCGGCG': 1791, 'TCGGGA': 1792, 'TCGGGT': 1793, 'TCGGGC': 1794, 'TCGGGG': 1795, 'TGAAAA': 1796, 'TGAAAT': 1797, 'TGAAAC': 1798, 'TGAAAG': 1799, 'TGAATA': 1800, 'TGAATT': 1801, 'TGAATC': 1802, 'TGAATG': 1803, 'TGAACA': 1804, 'TGAACT': 1805, 'TGAACC': 1806, 'TGAACG': 1807, 'TGAAGA': 1808, 'TGAAGT': 1809, 'TGAAGC': 1810, 'TGAAGG': 1811, 'TGATAA': 1812, 'TGATAT': 1813, 'TGATAC': 1814, 'TGATAG': 1815, 'TGATTA': 1816, 'TGATTT': 1817, 'TGATTC': 1818, 'TGATTG': 1819, 'TGATCA': 1820, 'TGATCT': 1821, 'TGATCC': 1822, 'TGATCG': 1823, 'TGATGA': 1824, 'TGATGT': 1825, 'TGATGC': 1826, 'TGATGG': 1827, 'TGACAA': 1828, 'TGACAT': 1829, 'TGACAC': 1830, 'TGACAG': 1831, 'TGACTA': 1832, 'TGACTT': 1833, 'TGACTC': 1834, 'TGACTG': 1835, 'TGACCA': 1836, 'TGACCT': 1837, 'TGACCC': 1838, 'TGACCG': 1839, 'TGACGA': 1840, 'TGACGT': 1841, 'TGACGC': 1842, 'TGACGG': 1843, 'TGAGAA': 1844, 'TGAGAT': 1845, 'TGAGAC': 1846, 'TGAGAG': 1847, 'TGAGTA': 1848, 'TGAGTT': 1849, 'TGAGTC': 1850, 'TGAGTG': 1851, 'TGAGCA': 1852, 'TGAGCT': 1853, 'TGAGCC': 1854, 'TGAGCG': 1855, 'TGAGGA': 1856, 'TGAGGT': 1857, 'TGAGGC': 1858, 'TGAGGG': 1859, 'TGTAAA': 1860, 'TGTAAT': 1861, 'TGTAAC': 1862, 'TGTAAG': 1863, 'TGTATA': 1864, 'TGTATT': 1865, 'TGTATC': 1866, 'TGTATG': 1867, 'TGTACA': 1868, 'TGTACT': 1869, 'TGTACC': 1870, 'TGTACG': 1871, 'TGTAGA': 1872, 'TGTAGT': 1873, 'TGTAGC': 1874, 'TGTAGG': 1875, 'TGTTAA': 1876, 'TGTTAT': 1877, 'TGTTAC': 1878, 'TGTTAG': 1879, 'TGTTTA': 1880, 'TGTTTT': 1881, 'TGTTTC': 1882, 'TGTTTG': 1883, 'TGTTCA': 1884, 'TGTTCT': 1885, 'TGTTCC': 1886, 'TGTTCG': 1887, 'TGTTGA': 1888, 'TGTTGT': 1889, 'TGTTGC': 1890, 'TGTTGG': 1891, 'TGTCAA': 1892, 'TGTCAT': 1893, 'TGTCAC': 1894, 'TGTCAG': 1895, 'TGTCTA': 1896, 'TGTCTT': 1897, 'TGTCTC': 1898, 'TGTCTG': 1899, 'TGTCCA': 1900, 'TGTCCT': 1901, 'TGTCCC': 1902, 'TGTCCG': 1903, 'TGTCGA': 1904, 'TGTCGT': 1905, 'TGTCGC': 1906, 'TGTCGG': 1907, 'TGTGAA': 1908, 'TGTGAT': 1909, 'TGTGAC': 1910, 'TGTGAG': 1911, 'TGTGTA': 1912, 'TGTGTT': 1913, 'TGTGTC': 1914, 'TGTGTG': 1915, 'TGTGCA': 1916, 'TGTGCT': 1917, 'TGTGCC': 1918, 'TGTGCG': 1919, 'TGTGGA': 1920, 'TGTGGT': 1921, 'TGTGGC': 1922, 'TGTGGG': 1923, 'TGCAAA': 1924, 'TGCAAT': 1925, 'TGCAAC': 1926, 'TGCAAG': 1927, 'TGCATA': 1928, 'TGCATT': 1929, 'TGCATC': 1930, 'TGCATG': 1931, 'TGCACA': 1932, 'TGCACT': 1933, 'TGCACC': 1934, 'TGCACG': 1935, 'TGCAGA': 1936, 'TGCAGT': 1937, 'TGCAGC': 1938, 'TGCAGG': 1939, 'TGCTAA': 1940, 'TGCTAT': 1941, 'TGCTAC': 1942, 'TGCTAG': 1943, 'TGCTTA': 1944, 'TGCTTT': 1945, 'TGCTTC': 1946, 'TGCTTG': 1947, 'TGCTCA': 1948, 'TGCTCT': 1949, 'TGCTCC': 1950, 'TGCTCG': 1951, 'TGCTGA': 1952, 'TGCTGT': 1953, 'TGCTGC': 1954, 'TGCTGG': 1955, 'TGCCAA': 1956, 'TGCCAT': 1957, 'TGCCAC': 1958, 'TGCCAG': 1959, 'TGCCTA': 1960, 'TGCCTT': 1961, 'TGCCTC': 1962, 'TGCCTG': 1963, 'TGCCCA': 1964, 'TGCCCT': 1965, 'TGCCCC': 1966, 'TGCCCG': 1967, 'TGCCGA': 1968, 'TGCCGT': 1969, 'TGCCGC': 1970, 'TGCCGG': 1971, 'TGCGAA': 1972, 'TGCGAT': 1973, 'TGCGAC': 1974, 'TGCGAG': 1975, 'TGCGTA': 1976, 'TGCGTT': 1977, 'TGCGTC': 1978, 'TGCGTG': 1979, 'TGCGCA': 1980, 'TGCGCT': 1981, 'TGCGCC': 1982, 'TGCGCG': 1983, 'TGCGGA': 1984, 'TGCGGT': 1985, 'TGCGGC': 1986, 'TGCGGG': 1987, 'TGGAAA': 1988, 'TGGAAT': 1989, 'TGGAAC': 1990, 'TGGAAG': 1991, 'TGGATA': 1992, 'TGGATT': 1993, 'TGGATC': 1994, 'TGGATG': 1995, 'TGGACA': 1996, 'TGGACT': 1997, 'TGGACC': 1998, 'TGGACG': 1999, 'TGGAGA': 2000, 'TGGAGT': 2001, 'TGGAGC': 2002, 'TGGAGG': 2003, 'TGGTAA': 2004, 'TGGTAT': 2005, 'TGGTAC': 2006, 'TGGTAG': 2007, 'TGGTTA': 2008, 'TGGTTT': 2009, 'TGGTTC': 2010, 'TGGTTG': 2011, 'TGGTCA': 2012, 'TGGTCT': 2013, 'TGGTCC': 2014, 'TGGTCG': 2015, 'TGGTGA': 2016, 'TGGTGT': 2017, 'TGGTGC': 2018, 'TGGTGG': 2019, 'TGGCAA': 2020, 'TGGCAT': 2021, 'TGGCAC': 2022, 'TGGCAG': 2023, 'TGGCTA': 2024, 'TGGCTT': 2025, 'TGGCTC': 2026, 'TGGCTG': 2027, 'TGGCCA': 2028, 'TGGCCT': 2029, 'TGGCCC': 2030, 'TGGCCG': 2031, 'TGGCGA': 2032, 'TGGCGT': 2033, 'TGGCGC': 2034, 'TGGCGG': 2035, 'TGGGAA': 2036, 'TGGGAT': 2037, 'TGGGAC': 2038, 'TGGGAG': 2039, 'TGGGTA': 2040, 'TGGGTT': 2041, 'TGGGTC': 2042, 'TGGGTG': 2043, 'TGGGCA': 2044, 'TGGGCT': 2045, 'TGGGCC': 2046, 'TGGGCG': 2047, 'TGGGGA': 2048, 'TGGGGT': 2049, 'TGGGGC': 2050, 'TGGGGG': 2051, 'CAAAAA': 2052, 'CAAAAT': 2053, 'CAAAAC': 2054, 'CAAAAG': 2055, 'CAAATA': 2056, 'CAAATT': 2057, 'CAAATC': 2058, 'CAAATG': 2059, 'CAAACA': 2060, 'CAAACT': 2061, 'CAAACC': 2062, 'CAAACG': 2063, 'CAAAGA': 2064, 'CAAAGT': 2065, 'CAAAGC': 2066, 'CAAAGG': 2067, 'CAATAA': 2068, 'CAATAT': 2069, 'CAATAC': 2070, 'CAATAG': 2071, 'CAATTA': 2072, 'CAATTT': 2073, 'CAATTC': 2074, 'CAATTG': 2075, 'CAATCA': 2076, 'CAATCT': 2077, 'CAATCC': 2078, 'CAATCG': 2079, 'CAATGA': 2080, 'CAATGT': 2081, 'CAATGC': 2082, 'CAATGG': 2083, 'CAACAA': 2084, 'CAACAT': 2085, 'CAACAC': 2086, 'CAACAG': 2087, 'CAACTA': 2088, 'CAACTT': 2089, 'CAACTC': 2090, 'CAACTG': 2091, 'CAACCA': 2092, 'CAACCT': 2093, 'CAACCC': 2094, 'CAACCG': 2095, 'CAACGA': 2096, 'CAACGT': 2097, 'CAACGC': 2098, 'CAACGG': 2099, 'CAAGAA': 2100, 'CAAGAT': 2101, 'CAAGAC': 2102, 'CAAGAG': 2103, 'CAAGTA': 2104, 'CAAGTT': 2105, 'CAAGTC': 2106, 'CAAGTG': 2107, 'CAAGCA': 2108, 'CAAGCT': 2109, 'CAAGCC': 2110, 'CAAGCG': 2111, 'CAAGGA': 2112, 'CAAGGT': 2113, 'CAAGGC': 2114, 'CAAGGG': 2115, 'CATAAA': 2116, 'CATAAT': 2117, 'CATAAC': 2118, 'CATAAG': 2119, 'CATATA': 2120, 'CATATT': 2121, 'CATATC': 2122, 'CATATG': 2123, 'CATACA': 2124, 'CATACT': 2125, 'CATACC': 2126, 'CATACG': 2127, 'CATAGA': 2128, 'CATAGT': 2129, 'CATAGC': 2130, 'CATAGG': 2131, 'CATTAA': 2132, 'CATTAT': 2133, 'CATTAC': 2134, 'CATTAG': 2135, 'CATTTA': 2136, 'CATTTT': 2137, 'CATTTC': 2138, 'CATTTG': 2139, 'CATTCA': 2140, 'CATTCT': 2141, 'CATTCC': 2142, 'CATTCG': 2143, 'CATTGA': 2144, 'CATTGT': 2145, 'CATTGC': 2146, 'CATTGG': 2147, 'CATCAA': 2148, 'CATCAT': 2149, 'CATCAC': 2150, 'CATCAG': 2151, 'CATCTA': 2152, 'CATCTT': 2153, 'CATCTC': 2154, 'CATCTG': 2155, 'CATCCA': 2156, 'CATCCT': 2157, 'CATCCC': 2158, 'CATCCG': 2159, 'CATCGA': 2160, 'CATCGT': 2161, 'CATCGC': 2162, 'CATCGG': 2163, 'CATGAA': 2164, 'CATGAT': 2165, 'CATGAC': 2166, 'CATGAG': 2167, 'CATGTA': 2168, 'CATGTT': 2169, 'CATGTC': 2170, 'CATGTG': 2171, 'CATGCA': 2172, 'CATGCT': 2173, 'CATGCC': 2174, 'CATGCG': 2175, 'CATGGA': 2176, 'CATGGT': 2177, 'CATGGC': 2178, 'CATGGG': 2179, 'CACAAA': 2180, 'CACAAT': 2181, 'CACAAC': 2182, 'CACAAG': 2183, 'CACATA': 2184, 'CACATT': 2185, 'CACATC': 2186, 'CACATG': 2187, 'CACACA': 2188, 'CACACT': 2189, 'CACACC': 2190, 'CACACG': 2191, 'CACAGA': 2192, 'CACAGT': 2193, 'CACAGC': 2194, 'CACAGG': 2195, 'CACTAA': 2196, 'CACTAT': 2197, 'CACTAC': 2198, 'CACTAG': 2199, 'CACTTA': 2200, 'CACTTT': 2201, 'CACTTC': 2202, 'CACTTG': 2203, 'CACTCA': 2204, 'CACTCT': 2205, 'CACTCC': 2206, 'CACTCG': 2207, 'CACTGA': 2208, 'CACTGT': 2209, 'CACTGC': 2210, 'CACTGG': 2211, 'CACCAA': 2212, 'CACCAT': 2213, 'CACCAC': 2214, 'CACCAG': 2215, 'CACCTA': 2216, 'CACCTT': 2217, 'CACCTC': 2218, 'CACCTG': 2219, 'CACCCA': 2220, 'CACCCT': 2221, 'CACCCC': 2222, 'CACCCG': 2223, 'CACCGA': 2224, 'CACCGT': 2225, 'CACCGC': 2226, 'CACCGG': 2227, 'CACGAA': 2228, 'CACGAT': 2229, 'CACGAC': 2230, 'CACGAG': 2231, 'CACGTA': 2232, 'CACGTT': 2233, 'CACGTC': 2234, 'CACGTG': 2235, 'CACGCA': 2236, 'CACGCT': 2237, 'CACGCC': 2238, 'CACGCG': 2239, 'CACGGA': 2240, 'CACGGT': 2241, 'CACGGC': 2242, 'CACGGG': 2243, 'CAGAAA': 2244, 'CAGAAT': 2245, 'CAGAAC': 2246, 'CAGAAG': 2247, 'CAGATA': 2248, 'CAGATT': 2249, 'CAGATC': 2250, 'CAGATG': 2251, 'CAGACA': 2252, 'CAGACT': 2253, 'CAGACC': 2254, 'CAGACG': 2255, 'CAGAGA': 2256, 'CAGAGT': 2257, 'CAGAGC': 2258, 'CAGAGG': 2259, 'CAGTAA': 2260, 'CAGTAT': 2261, 'CAGTAC': 2262, 'CAGTAG': 2263, 'CAGTTA': 2264, 'CAGTTT': 2265, 'CAGTTC': 2266, 'CAGTTG': 2267, 'CAGTCA': 2268, 'CAGTCT': 2269, 'CAGTCC': 2270, 'CAGTCG': 2271, 'CAGTGA': 2272, 'CAGTGT': 2273, 'CAGTGC': 2274, 'CAGTGG': 2275, 'CAGCAA': 2276, 'CAGCAT': 2277, 'CAGCAC': 2278, 'CAGCAG': 2279, 'CAGCTA': 2280, 'CAGCTT': 2281, 'CAGCTC': 2282, 'CAGCTG': 2283, 'CAGCCA': 2284, 'CAGCCT': 2285, 'CAGCCC': 2286, 'CAGCCG': 2287, 'CAGCGA': 2288, 'CAGCGT': 2289, 'CAGCGC': 2290, 'CAGCGG': 2291, 'CAGGAA': 2292, 'CAGGAT': 2293, 'CAGGAC': 2294, 'CAGGAG': 2295, 'CAGGTA': 2296, 'CAGGTT': 2297, 'CAGGTC': 2298, 'CAGGTG': 2299, 'CAGGCA': 2300, 'CAGGCT': 2301, 'CAGGCC': 2302, 'CAGGCG': 2303, 'CAGGGA': 2304, 'CAGGGT': 2305, 'CAGGGC': 2306, 'CAGGGG': 2307, 'CTAAAA': 2308, 'CTAAAT': 2309, 'CTAAAC': 2310, 'CTAAAG': 2311, 'CTAATA': 2312, 'CTAATT': 2313, 'CTAATC': 2314, 'CTAATG': 2315, 'CTAACA': 2316, 'CTAACT': 2317, 'CTAACC': 2318, 'CTAACG': 2319, 'CTAAGA': 2320, 'CTAAGT': 2321, 'CTAAGC': 2322, 'CTAAGG': 2323, 'CTATAA': 2324, 'CTATAT': 2325, 'CTATAC': 2326, 'CTATAG': 2327, 'CTATTA': 2328, 'CTATTT': 2329, 'CTATTC': 2330, 'CTATTG': 2331, 'CTATCA': 2332, 'CTATCT': 2333, 'CTATCC': 2334, 'CTATCG': 2335, 'CTATGA': 2336, 'CTATGT': 2337, 'CTATGC': 2338, 'CTATGG': 2339, 'CTACAA': 2340, 'CTACAT': 2341, 'CTACAC': 2342, 'CTACAG': 2343, 'CTACTA': 2344, 'CTACTT': 2345, 'CTACTC': 2346, 'CTACTG': 2347, 'CTACCA': 2348, 'CTACCT': 2349, 'CTACCC': 2350, 'CTACCG': 2351, 'CTACGA': 2352, 'CTACGT': 2353, 'CTACGC': 2354, 'CTACGG': 2355, 'CTAGAA': 2356, 'CTAGAT': 2357, 'CTAGAC': 2358, 'CTAGAG': 2359, 'CTAGTA': 2360, 'CTAGTT': 2361, 'CTAGTC': 2362, 'CTAGTG': 2363, 'CTAGCA': 2364, 'CTAGCT': 2365, 'CTAGCC': 2366, 'CTAGCG': 2367, 'CTAGGA': 2368, 'CTAGGT': 2369, 'CTAGGC': 2370, 'CTAGGG': 2371, 'CTTAAA': 2372, 'CTTAAT': 2373, 'CTTAAC': 2374, 'CTTAAG': 2375, 'CTTATA': 2376, 'CTTATT': 2377, 'CTTATC': 2378, 'CTTATG': 2379, 'CTTACA': 2380, 'CTTACT': 2381, 'CTTACC': 2382, 'CTTACG': 2383, 'CTTAGA': 2384, 'CTTAGT': 2385, 'CTTAGC': 2386, 'CTTAGG': 2387, 'CTTTAA': 2388, 'CTTTAT': 2389, 'CTTTAC': 2390, 'CTTTAG': 2391, 'CTTTTA': 2392, 'CTTTTT': 2393, 'CTTTTC': 2394, 'CTTTTG': 2395, 'CTTTCA': 2396, 'CTTTCT': 2397, 'CTTTCC': 2398, 'CTTTCG': 2399, 'CTTTGA': 2400, 'CTTTGT': 2401, 'CTTTGC': 2402, 'CTTTGG': 2403, 'CTTCAA': 2404, 'CTTCAT': 2405, 'CTTCAC': 2406, 'CTTCAG': 2407, 'CTTCTA': 2408, 'CTTCTT': 2409, 'CTTCTC': 2410, 'CTTCTG': 2411, 'CTTCCA': 2412, 'CTTCCT': 2413, 'CTTCCC': 2414, 'CTTCCG': 2415, 'CTTCGA': 2416, 'CTTCGT': 2417, 'CTTCGC': 2418, 'CTTCGG': 2419, 'CTTGAA': 2420, 'CTTGAT': 2421, 'CTTGAC': 2422, 'CTTGAG': 2423, 'CTTGTA': 2424, 'CTTGTT': 2425, 'CTTGTC': 2426, 'CTTGTG': 2427, 'CTTGCA': 2428, 'CTTGCT': 2429, 'CTTGCC': 2430, 'CTTGCG': 2431, 'CTTGGA': 2432, 'CTTGGT': 2433, 'CTTGGC': 2434, 'CTTGGG': 2435, 'CTCAAA': 2436, 'CTCAAT': 2437, 'CTCAAC': 2438, 'CTCAAG': 2439, 'CTCATA': 2440, 'CTCATT': 2441, 'CTCATC': 2442, 'CTCATG': 2443, 'CTCACA': 2444, 'CTCACT': 2445, 'CTCACC': 2446, 'CTCACG': 2447, 'CTCAGA': 2448, 'CTCAGT': 2449, 'CTCAGC': 2450, 'CTCAGG': 2451, 'CTCTAA': 2452, 'CTCTAT': 2453, 'CTCTAC': 2454, 'CTCTAG': 2455, 'CTCTTA': 2456, 'CTCTTT': 2457, 'CTCTTC': 2458, 'CTCTTG': 2459, 'CTCTCA': 2460, 'CTCTCT': 2461, 'CTCTCC': 2462, 'CTCTCG': 2463, 'CTCTGA': 2464, 'CTCTGT': 2465, 'CTCTGC': 2466, 'CTCTGG': 2467, 'CTCCAA': 2468, 'CTCCAT': 2469, 'CTCCAC': 2470, 'CTCCAG': 2471, 'CTCCTA': 2472, 'CTCCTT': 2473, 'CTCCTC': 2474, 'CTCCTG': 2475, 'CTCCCA': 2476, 'CTCCCT': 2477, 'CTCCCC': 2478, 'CTCCCG': 2479, 'CTCCGA': 2480, 'CTCCGT': 2481, 'CTCCGC': 2482, 'CTCCGG': 2483, 'CTCGAA': 2484, 'CTCGAT': 2485, 'CTCGAC': 2486, 'CTCGAG': 2487, 'CTCGTA': 2488, 'CTCGTT': 2489, 'CTCGTC': 2490, 'CTCGTG': 2491, 'CTCGCA': 2492, 'CTCGCT': 2493, 'CTCGCC': 2494, 'CTCGCG': 2495, 'CTCGGA': 2496, 'CTCGGT': 2497, 'CTCGGC': 2498, 'CTCGGG': 2499, 'CTGAAA': 2500, 'CTGAAT': 2501, 'CTGAAC': 2502, 'CTGAAG': 2503, 'CTGATA': 2504, 'CTGATT': 2505, 'CTGATC': 2506, 'CTGATG': 2507, 'CTGACA': 2508, 'CTGACT': 2509, 'CTGACC': 2510, 'CTGACG': 2511, 'CTGAGA': 2512, 'CTGAGT': 2513, 'CTGAGC': 2514, 'CTGAGG': 2515, 'CTGTAA': 2516, 'CTGTAT': 2517, 'CTGTAC': 2518, 'CTGTAG': 2519, 'CTGTTA': 2520, 'CTGTTT': 2521, 'CTGTTC': 2522, 'CTGTTG': 2523, 'CTGTCA': 2524, 'CTGTCT': 2525, 'CTGTCC': 2526, 'CTGTCG': 2527, 'CTGTGA': 2528, 'CTGTGT': 2529, 'CTGTGC': 2530, 'CTGTGG': 2531, 'CTGCAA': 2532, 'CTGCAT': 2533, 'CTGCAC': 2534, 'CTGCAG': 2535, 'CTGCTA': 2536, 'CTGCTT': 2537, 'CTGCTC': 2538, 'CTGCTG': 2539, 'CTGCCA': 2540, 'CTGCCT': 2541, 'CTGCCC': 2542, 'CTGCCG': 2543, 'CTGCGA': 2544, 'CTGCGT': 2545, 'CTGCGC': 2546, 'CTGCGG': 2547, 'CTGGAA': 2548, 'CTGGAT': 2549, 'CTGGAC': 2550, 'CTGGAG': 2551, 'CTGGTA': 2552, 'CTGGTT': 2553, 'CTGGTC': 2554, 'CTGGTG': 2555, 'CTGGCA': 2556, 'CTGGCT': 2557, 'CTGGCC': 2558, 'CTGGCG': 2559, 'CTGGGA': 2560, 'CTGGGT': 2561, 'CTGGGC': 2562, 'CTGGGG': 2563, 'CCAAAA': 2564, 'CCAAAT': 2565, 'CCAAAC': 2566, 'CCAAAG': 2567, 'CCAATA': 2568, 'CCAATT': 2569, 'CCAATC': 2570, 'CCAATG': 2571, 'CCAACA': 2572, 'CCAACT': 2573, 'CCAACC': 2574, 'CCAACG': 2575, 'CCAAGA': 2576, 'CCAAGT': 2577, 'CCAAGC': 2578, 'CCAAGG': 2579, 'CCATAA': 2580, 'CCATAT': 2581, 'CCATAC': 2582, 'CCATAG': 2583, 'CCATTA': 2584, 'CCATTT': 2585, 'CCATTC': 2586, 'CCATTG': 2587, 'CCATCA': 2588, 'CCATCT': 2589, 'CCATCC': 2590, 'CCATCG': 2591, 'CCATGA': 2592, 'CCATGT': 2593, 'CCATGC': 2594, 'CCATGG': 2595, 'CCACAA': 2596, 'CCACAT': 2597, 'CCACAC': 2598, 'CCACAG': 2599, 'CCACTA': 2600, 'CCACTT': 2601, 'CCACTC': 2602, 'CCACTG': 2603, 'CCACCA': 2604, 'CCACCT': 2605, 'CCACCC': 2606, 'CCACCG': 2607, 'CCACGA': 2608, 'CCACGT': 2609, 'CCACGC': 2610, 'CCACGG': 2611, 'CCAGAA': 2612, 'CCAGAT': 2613, 'CCAGAC': 2614, 'CCAGAG': 2615, 'CCAGTA': 2616, 'CCAGTT': 2617, 'CCAGTC': 2618, 'CCAGTG': 2619, 'CCAGCA': 2620, 'CCAGCT': 2621, 'CCAGCC': 2622, 'CCAGCG': 2623, 'CCAGGA': 2624, 'CCAGGT': 2625, 'CCAGGC': 2626, 'CCAGGG': 2627, 'CCTAAA': 2628, 'CCTAAT': 2629, 'CCTAAC': 2630, 'CCTAAG': 2631, 'CCTATA': 2632, 'CCTATT': 2633, 'CCTATC': 2634, 'CCTATG': 2635, 'CCTACA': 2636, 'CCTACT': 2637, 'CCTACC': 2638, 'CCTACG': 2639, 'CCTAGA': 2640, 'CCTAGT': 2641, 'CCTAGC': 2642, 'CCTAGG': 2643, 'CCTTAA': 2644, 'CCTTAT': 2645, 'CCTTAC': 2646, 'CCTTAG': 2647, 'CCTTTA': 2648, 'CCTTTT': 2649, 'CCTTTC': 2650, 'CCTTTG': 2651, 'CCTTCA': 2652, 'CCTTCT': 2653, 'CCTTCC': 2654, 'CCTTCG': 2655, 'CCTTGA': 2656, 'CCTTGT': 2657, 'CCTTGC': 2658, 'CCTTGG': 2659, 'CCTCAA': 2660, 'CCTCAT': 2661, 'CCTCAC': 2662, 'CCTCAG': 2663, 'CCTCTA': 2664, 'CCTCTT': 2665, 'CCTCTC': 2666, 'CCTCTG': 2667, 'CCTCCA': 2668, 'CCTCCT': 2669, 'CCTCCC': 2670, 'CCTCCG': 2671, 'CCTCGA': 2672, 'CCTCGT': 2673, 'CCTCGC': 2674, 'CCTCGG': 2675, 'CCTGAA': 2676, 'CCTGAT': 2677, 'CCTGAC': 2678, 'CCTGAG': 2679, 'CCTGTA': 2680, 'CCTGTT': 2681, 'CCTGTC': 2682, 'CCTGTG': 2683, 'CCTGCA': 2684, 'CCTGCT': 2685, 'CCTGCC': 2686, 'CCTGCG': 2687, 'CCTGGA': 2688, 'CCTGGT': 2689, 'CCTGGC': 2690, 'CCTGGG': 2691, 'CCCAAA': 2692, 'CCCAAT': 2693, 'CCCAAC': 2694, 'CCCAAG': 2695, 'CCCATA': 2696, 'CCCATT': 2697, 'CCCATC': 2698, 'CCCATG': 2699, 'CCCACA': 2700, 'CCCACT': 2701, 'CCCACC': 2702, 'CCCACG': 2703, 'CCCAGA': 2704, 'CCCAGT': 2705, 'CCCAGC': 2706, 'CCCAGG': 2707, 'CCCTAA': 2708, 'CCCTAT': 2709, 'CCCTAC': 2710, 'CCCTAG': 2711, 'CCCTTA': 2712, 'CCCTTT': 2713, 'CCCTTC': 2714, 'CCCTTG': 2715, 'CCCTCA': 2716, 'CCCTCT': 2717, 'CCCTCC': 2718, 'CCCTCG': 2719, 'CCCTGA': 2720, 'CCCTGT': 2721, 'CCCTGC': 2722, 'CCCTGG': 2723, 'CCCCAA': 2724, 'CCCCAT': 2725, 'CCCCAC': 2726, 'CCCCAG': 2727, 'CCCCTA': 2728, 'CCCCTT': 2729, 'CCCCTC': 2730, 'CCCCTG': 2731, 'CCCCCA': 2732, 'CCCCCT': 2733, 'CCCCCC': 2734, 'CCCCCG': 2735, 'CCCCGA': 2736, 'CCCCGT': 2737, 'CCCCGC': 2738, 'CCCCGG': 2739, 'CCCGAA': 2740, 'CCCGAT': 2741, 'CCCGAC': 2742, 'CCCGAG': 2743, 'CCCGTA': 2744, 'CCCGTT': 2745, 'CCCGTC': 2746, 'CCCGTG': 2747, 'CCCGCA': 2748, 'CCCGCT': 2749, 'CCCGCC': 2750, 'CCCGCG': 2751, 'CCCGGA': 2752, 'CCCGGT': 2753, 'CCCGGC': 2754, 'CCCGGG': 2755, 'CCGAAA': 2756, 'CCGAAT': 2757, 'CCGAAC': 2758, 'CCGAAG': 2759, 'CCGATA': 2760, 'CCGATT': 2761, 'CCGATC': 2762, 'CCGATG': 2763, 'CCGACA': 2764, 'CCGACT': 2765, 'CCGACC': 2766, 'CCGACG': 2767, 'CCGAGA': 2768, 'CCGAGT': 2769, 'CCGAGC': 2770, 'CCGAGG': 2771, 'CCGTAA': 2772, 'CCGTAT': 2773, 'CCGTAC': 2774, 'CCGTAG': 2775, 'CCGTTA': 2776, 'CCGTTT': 2777, 'CCGTTC': 2778, 'CCGTTG': 2779, 'CCGTCA': 2780, 'CCGTCT': 2781, 'CCGTCC': 2782, 'CCGTCG': 2783, 'CCGTGA': 2784, 'CCGTGT': 2785, 'CCGTGC': 2786, 'CCGTGG': 2787, 'CCGCAA': 2788, 'CCGCAT': 2789, 'CCGCAC': 2790, 'CCGCAG': 2791, 'CCGCTA': 2792, 'CCGCTT': 2793, 'CCGCTC': 2794, 'CCGCTG': 2795, 'CCGCCA': 2796, 'CCGCCT': 2797, 'CCGCCC': 2798, 'CCGCCG': 2799, 'CCGCGA': 2800, 'CCGCGT': 2801, 'CCGCGC': 2802, 'CCGCGG': 2803, 'CCGGAA': 2804, 'CCGGAT': 2805, 'CCGGAC': 2806, 'CCGGAG': 2807, 'CCGGTA': 2808, 'CCGGTT': 2809, 'CCGGTC': 2810, 'CCGGTG': 2811, 'CCGGCA': 2812, 'CCGGCT': 2813, 'CCGGCC': 2814, 'CCGGCG': 2815, 'CCGGGA': 2816, 'CCGGGT': 2817, 'CCGGGC': 2818, 'CCGGGG': 2819, 'CGAAAA': 2820, 'CGAAAT': 2821, 'CGAAAC': 2822, 'CGAAAG': 2823, 'CGAATA': 2824, 'CGAATT': 2825, 'CGAATC': 2826, 'CGAATG': 2827, 'CGAACA': 2828, 'CGAACT': 2829, 'CGAACC': 2830, 'CGAACG': 2831, 'CGAAGA': 2832, 'CGAAGT': 2833, 'CGAAGC': 2834, 'CGAAGG': 2835, 'CGATAA': 2836, 'CGATAT': 2837, 'CGATAC': 2838, 'CGATAG': 2839, 'CGATTA': 2840, 'CGATTT': 2841, 'CGATTC': 2842, 'CGATTG': 2843, 'CGATCA': 2844, 'CGATCT': 2845, 'CGATCC': 2846, 'CGATCG': 2847, 'CGATGA': 2848, 'CGATGT': 2849, 'CGATGC': 2850, 'CGATGG': 2851, 'CGACAA': 2852, 'CGACAT': 2853, 'CGACAC': 2854, 'CGACAG': 2855, 'CGACTA': 2856, 'CGACTT': 2857, 'CGACTC': 2858, 'CGACTG': 2859, 'CGACCA': 2860, 'CGACCT': 2861, 'CGACCC': 2862, 'CGACCG': 2863, 'CGACGA': 2864, 'CGACGT': 2865, 'CGACGC': 2866, 'CGACGG': 2867, 'CGAGAA': 2868, 'CGAGAT': 2869, 'CGAGAC': 2870, 'CGAGAG': 2871, 'CGAGTA': 2872, 'CGAGTT': 2873, 'CGAGTC': 2874, 'CGAGTG': 2875, 'CGAGCA': 2876, 'CGAGCT': 2877, 'CGAGCC': 2878, 'CGAGCG': 2879, 'CGAGGA': 2880, 'CGAGGT': 2881, 'CGAGGC': 2882, 'CGAGGG': 2883, 'CGTAAA': 2884, 'CGTAAT': 2885, 'CGTAAC': 2886, 'CGTAAG': 2887, 'CGTATA': 2888, 'CGTATT': 2889, 'CGTATC': 2890, 'CGTATG': 2891, 'CGTACA': 2892, 'CGTACT': 2893, 'CGTACC': 2894, 'CGTACG': 2895, 'CGTAGA': 2896, 'CGTAGT': 2897, 'CGTAGC': 2898, 'CGTAGG': 2899, 'CGTTAA': 2900, 'CGTTAT': 2901, 'CGTTAC': 2902, 'CGTTAG': 2903, 'CGTTTA': 2904, 'CGTTTT': 2905, 'CGTTTC': 2906, 'CGTTTG': 2907, 'CGTTCA': 2908, 'CGTTCT': 2909, 'CGTTCC': 2910, 'CGTTCG': 2911, 'CGTTGA': 2912, 'CGTTGT': 2913, 'CGTTGC': 2914, 'CGTTGG': 2915, 'CGTCAA': 2916, 'CGTCAT': 2917, 'CGTCAC': 2918, 'CGTCAG': 2919, 'CGTCTA': 2920, 'CGTCTT': 2921, 'CGTCTC': 2922, 'CGTCTG': 2923, 'CGTCCA': 2924, 'CGTCCT': 2925, 'CGTCCC': 2926, 'CGTCCG': 2927, 'CGTCGA': 2928, 'CGTCGT': 2929, 'CGTCGC': 2930, 'CGTCGG': 2931, 'CGTGAA': 2932, 'CGTGAT': 2933, 'CGTGAC': 2934, 'CGTGAG': 2935, 'CGTGTA': 2936, 'CGTGTT': 2937, 'CGTGTC': 2938, 'CGTGTG': 2939, 'CGTGCA': 2940, 'CGTGCT': 2941, 'CGTGCC': 2942, 'CGTGCG': 2943, 'CGTGGA': 2944, 'CGTGGT': 2945, 'CGTGGC': 2946, 'CGTGGG': 2947, 'CGCAAA': 2948, 'CGCAAT': 2949, 'CGCAAC': 2950, 'CGCAAG': 2951, 'CGCATA': 2952, 'CGCATT': 2953, 'CGCATC': 2954, 'CGCATG': 2955, 'CGCACA': 2956, 'CGCACT': 2957, 'CGCACC': 2958, 'CGCACG': 2959, 'CGCAGA': 2960, 'CGCAGT': 2961, 'CGCAGC': 2962, 'CGCAGG': 2963, 'CGCTAA': 2964, 'CGCTAT': 2965, 'CGCTAC': 2966, 'CGCTAG': 2967, 'CGCTTA': 2968, 'CGCTTT': 2969, 'CGCTTC': 2970, 'CGCTTG': 2971, 'CGCTCA': 2972, 'CGCTCT': 2973, 'CGCTCC': 2974, 'CGCTCG': 2975, 'CGCTGA': 2976, 'CGCTGT': 2977, 'CGCTGC': 2978, 'CGCTGG': 2979, 'CGCCAA': 2980, 'CGCCAT': 2981, 'CGCCAC': 2982, 'CGCCAG': 2983, 'CGCCTA': 2984, 'CGCCTT': 2985, 'CGCCTC': 2986, 'CGCCTG': 2987, 'CGCCCA': 2988, 'CGCCCT': 2989, 'CGCCCC': 2990, 'CGCCCG': 2991, 'CGCCGA': 2992, 'CGCCGT': 2993, 'CGCCGC': 2994, 'CGCCGG': 2995, 'CGCGAA': 2996, 'CGCGAT': 2997, 'CGCGAC': 2998, 'CGCGAG': 2999, 'CGCGTA': 3000, 'CGCGTT': 3001, 'CGCGTC': 3002, 'CGCGTG': 3003, 'CGCGCA': 3004, 'CGCGCT': 3005, 'CGCGCC': 3006, 'CGCGCG': 3007, 'CGCGGA': 3008, 'CGCGGT': 3009, 'CGCGGC': 3010, 'CGCGGG': 3011, 'CGGAAA': 3012, 'CGGAAT': 3013, 'CGGAAC': 3014, 'CGGAAG': 3015, 'CGGATA': 3016, 'CGGATT': 3017, 'CGGATC': 3018, 'CGGATG': 3019, 'CGGACA': 3020, 'CGGACT': 3021, 'CGGACC': 3022, 'CGGACG': 3023, 'CGGAGA': 3024, 'CGGAGT': 3025, 'CGGAGC': 3026, 'CGGAGG': 3027, 'CGGTAA': 3028, 'CGGTAT': 3029, 'CGGTAC': 3030, 'CGGTAG': 3031, 'CGGTTA': 3032, 'CGGTTT': 3033, 'CGGTTC': 3034, 'CGGTTG': 3035, 'CGGTCA': 3036, 'CGGTCT': 3037, 'CGGTCC': 3038, 'CGGTCG': 3039, 'CGGTGA': 3040, 'CGGTGT': 3041, 'CGGTGC': 3042, 'CGGTGG': 3043, 'CGGCAA': 3044, 'CGGCAT': 3045, 'CGGCAC': 3046, 'CGGCAG': 3047, 'CGGCTA': 3048, 'CGGCTT': 3049, 'CGGCTC': 3050, 'CGGCTG': 3051, 'CGGCCA': 3052, 'CGGCCT': 3053, 'CGGCCC': 3054, 'CGGCCG': 3055, 'CGGCGA': 3056, 'CGGCGT': 3057, 'CGGCGC': 3058, 'CGGCGG': 3059, 'CGGGAA': 3060, 'CGGGAT': 3061, 'CGGGAC': 3062, 'CGGGAG': 3063, 'CGGGTA': 3064, 'CGGGTT': 3065, 'CGGGTC': 3066, 'CGGGTG': 3067, 'CGGGCA': 3068, 'CGGGCT': 3069, 'CGGGCC': 3070, 'CGGGCG': 3071, 'CGGGGA': 3072, 'CGGGGT': 3073, 'CGGGGC': 3074, 'CGGGGG': 3075, 'GAAAAA': 3076, 'GAAAAT': 3077, 'GAAAAC': 3078, 'GAAAAG': 3079, 'GAAATA': 3080, 'GAAATT': 3081, 'GAAATC': 3082, 'GAAATG': 3083, 'GAAACA': 3084, 'GAAACT': 3085, 'GAAACC': 3086, 'GAAACG': 3087, 'GAAAGA': 3088, 'GAAAGT': 3089, 'GAAAGC': 3090, 'GAAAGG': 3091, 'GAATAA': 3092, 'GAATAT': 3093, 'GAATAC': 3094, 'GAATAG': 3095, 'GAATTA': 3096, 'GAATTT': 3097, 'GAATTC': 3098, 'GAATTG': 3099, 'GAATCA': 3100, 'GAATCT': 3101, 'GAATCC': 3102, 'GAATCG': 3103, 'GAATGA': 3104, 'GAATGT': 3105, 'GAATGC': 3106, 'GAATGG': 3107, 'GAACAA': 3108, 'GAACAT': 3109, 'GAACAC': 3110, 'GAACAG': 3111, 'GAACTA': 3112, 'GAACTT': 3113, 'GAACTC': 3114, 'GAACTG': 3115, 'GAACCA': 3116, 'GAACCT': 3117, 'GAACCC': 3118, 'GAACCG': 3119, 'GAACGA': 3120, 'GAACGT': 3121, 'GAACGC': 3122, 'GAACGG': 3123, 'GAAGAA': 3124, 'GAAGAT': 3125, 'GAAGAC': 3126, 'GAAGAG': 3127, 'GAAGTA': 3128, 'GAAGTT': 3129, 'GAAGTC': 3130, 'GAAGTG': 3131, 'GAAGCA': 3132, 'GAAGCT': 3133, 'GAAGCC': 3134, 'GAAGCG': 3135, 'GAAGGA': 3136, 'GAAGGT': 3137, 'GAAGGC': 3138, 'GAAGGG': 3139, 'GATAAA': 3140, 'GATAAT': 3141, 'GATAAC': 3142, 'GATAAG': 3143, 'GATATA': 3144, 'GATATT': 3145, 'GATATC': 3146, 'GATATG': 3147, 'GATACA': 3148, 'GATACT': 3149, 'GATACC': 3150, 'GATACG': 3151, 'GATAGA': 3152, 'GATAGT': 3153, 'GATAGC': 3154, 'GATAGG': 3155, 'GATTAA': 3156, 'GATTAT': 3157, 'GATTAC': 3158, 'GATTAG': 3159, 'GATTTA': 3160, 'GATTTT': 3161, 'GATTTC': 3162, 'GATTTG': 3163, 'GATTCA': 3164, 'GATTCT': 3165, 'GATTCC': 3166, 'GATTCG': 3167, 'GATTGA': 3168, 'GATTGT': 3169, 'GATTGC': 3170, 'GATTGG': 3171, 'GATCAA': 3172, 'GATCAT': 3173, 'GATCAC': 3174, 'GATCAG': 3175, 'GATCTA': 3176, 'GATCTT': 3177, 'GATCTC': 3178, 'GATCTG': 3179, 'GATCCA': 3180, 'GATCCT': 3181, 'GATCCC': 3182, 'GATCCG': 3183, 'GATCGA': 3184, 'GATCGT': 3185, 'GATCGC': 3186, 'GATCGG': 3187, 'GATGAA': 3188, 'GATGAT': 3189, 'GATGAC': 3190, 'GATGAG': 3191, 'GATGTA': 3192, 'GATGTT': 3193, 'GATGTC': 3194, 'GATGTG': 3195, 'GATGCA': 3196, 'GATGCT': 3197, 'GATGCC': 3198, 'GATGCG': 3199, 'GATGGA': 3200, 'GATGGT': 3201, 'GATGGC': 3202, 'GATGGG': 3203, 'GACAAA': 3204, 'GACAAT': 3205, 'GACAAC': 3206, 'GACAAG': 3207, 'GACATA': 3208, 'GACATT': 3209, 'GACATC': 3210, 'GACATG': 3211, 'GACACA': 3212, 'GACACT': 3213, 'GACACC': 3214, 'GACACG': 3215, 'GACAGA': 3216, 'GACAGT': 3217, 'GACAGC': 3218, 'GACAGG': 3219, 'GACTAA': 3220, 'GACTAT': 3221, 'GACTAC': 3222, 'GACTAG': 3223, 'GACTTA': 3224, 'GACTTT': 3225, 'GACTTC': 3226, 'GACTTG': 3227, 'GACTCA': 3228, 'GACTCT': 3229, 'GACTCC': 3230, 'GACTCG': 3231, 'GACTGA': 3232, 'GACTGT': 3233, 'GACTGC': 3234, 'GACTGG': 3235, 'GACCAA': 3236, 'GACCAT': 3237, 'GACCAC': 3238, 'GACCAG': 3239, 'GACCTA': 3240, 'GACCTT': 3241, 'GACCTC': 3242, 'GACCTG': 3243, 'GACCCA': 3244, 'GACCCT': 3245, 'GACCCC': 3246, 'GACCCG': 3247, 'GACCGA': 3248, 'GACCGT': 3249, 'GACCGC': 3250, 'GACCGG': 3251, 'GACGAA': 3252, 'GACGAT': 3253, 'GACGAC': 3254, 'GACGAG': 3255, 'GACGTA': 3256, 'GACGTT': 3257, 'GACGTC': 3258, 'GACGTG': 3259, 'GACGCA': 3260, 'GACGCT': 3261, 'GACGCC': 3262, 'GACGCG': 3263, 'GACGGA': 3264, 'GACGGT': 3265, 'GACGGC': 3266, 'GACGGG': 3267, 'GAGAAA': 3268, 'GAGAAT': 3269, 'GAGAAC': 3270, 'GAGAAG': 3271, 'GAGATA': 3272, 'GAGATT': 3273, 'GAGATC': 3274, 'GAGATG': 3275, 'GAGACA': 3276, 'GAGACT': 3277, 'GAGACC': 3278, 'GAGACG': 3279, 'GAGAGA': 3280, 'GAGAGT': 3281, 'GAGAGC': 3282, 'GAGAGG': 3283, 'GAGTAA': 3284, 'GAGTAT': 3285, 'GAGTAC': 3286, 'GAGTAG': 3287, 'GAGTTA': 3288, 'GAGTTT': 3289, 'GAGTTC': 3290, 'GAGTTG': 3291, 'GAGTCA': 3292, 'GAGTCT': 3293, 'GAGTCC': 3294, 'GAGTCG': 3295, 'GAGTGA': 3296, 'GAGTGT': 3297, 'GAGTGC': 3298, 'GAGTGG': 3299, 'GAGCAA': 3300, 'GAGCAT': 3301, 'GAGCAC': 3302, 'GAGCAG': 3303, 'GAGCTA': 3304, 'GAGCTT': 3305, 'GAGCTC': 3306, 'GAGCTG': 3307, 'GAGCCA': 3308, 'GAGCCT': 3309, 'GAGCCC': 3310, 'GAGCCG': 3311, 'GAGCGA': 3312, 'GAGCGT': 3313, 'GAGCGC': 3314, 'GAGCGG': 3315, 'GAGGAA': 3316, 'GAGGAT': 3317, 'GAGGAC': 3318, 'GAGGAG': 3319, 'GAGGTA': 3320, 'GAGGTT': 3321, 'GAGGTC': 3322, 'GAGGTG': 3323, 'GAGGCA': 3324, 'GAGGCT': 3325, 'GAGGCC': 3326, 'GAGGCG': 3327, 'GAGGGA': 3328, 'GAGGGT': 3329, 'GAGGGC': 3330, 'GAGGGG': 3331, 'GTAAAA': 3332, 'GTAAAT': 3333, 'GTAAAC': 3334, 'GTAAAG': 3335, 'GTAATA': 3336, 'GTAATT': 3337, 'GTAATC': 3338, 'GTAATG': 3339, 'GTAACA': 3340, 'GTAACT': 3341, 'GTAACC': 3342, 'GTAACG': 3343, 'GTAAGA': 3344, 'GTAAGT': 3345, 'GTAAGC': 3346, 'GTAAGG': 3347, 'GTATAA': 3348, 'GTATAT': 3349, 'GTATAC': 3350, 'GTATAG': 3351, 'GTATTA': 3352, 'GTATTT': 3353, 'GTATTC': 3354, 'GTATTG': 3355, 'GTATCA': 3356, 'GTATCT': 3357, 'GTATCC': 3358, 'GTATCG': 3359, 'GTATGA': 3360, 'GTATGT': 3361, 'GTATGC': 3362, 'GTATGG': 3363, 'GTACAA': 3364, 'GTACAT': 3365, 'GTACAC': 3366, 'GTACAG': 3367, 'GTACTA': 3368, 'GTACTT': 3369, 'GTACTC': 3370, 'GTACTG': 3371, 'GTACCA': 3372, 'GTACCT': 3373, 'GTACCC': 3374, 'GTACCG': 3375, 'GTACGA': 3376, 'GTACGT': 3377, 'GTACGC': 3378, 'GTACGG': 3379, 'GTAGAA': 3380, 'GTAGAT': 3381, 'GTAGAC': 3382, 'GTAGAG': 3383, 'GTAGTA': 3384, 'GTAGTT': 3385, 'GTAGTC': 3386, 'GTAGTG': 3387, 'GTAGCA': 3388, 'GTAGCT': 3389, 'GTAGCC': 3390, 'GTAGCG': 3391, 'GTAGGA': 3392, 'GTAGGT': 3393, 'GTAGGC': 3394, 'GTAGGG': 3395, 'GTTAAA': 3396, 'GTTAAT': 3397, 'GTTAAC': 3398, 'GTTAAG': 3399, 'GTTATA': 3400, 'GTTATT': 3401, 'GTTATC': 3402, 'GTTATG': 3403, 'GTTACA': 3404, 'GTTACT': 3405, 'GTTACC': 3406, 'GTTACG': 3407, 'GTTAGA': 3408, 'GTTAGT': 3409, 'GTTAGC': 3410, 'GTTAGG': 3411, 'GTTTAA': 3412, 'GTTTAT': 3413, 'GTTTAC': 3414, 'GTTTAG': 3415, 'GTTTTA': 3416, 'GTTTTT': 3417, 'GTTTTC': 3418, 'GTTTTG': 3419, 'GTTTCA': 3420, 'GTTTCT': 3421, 'GTTTCC': 3422, 'GTTTCG': 3423, 'GTTTGA': 3424, 'GTTTGT': 3425, 'GTTTGC': 3426, 'GTTTGG': 3427, 'GTTCAA': 3428, 'GTTCAT': 3429, 'GTTCAC': 3430, 'GTTCAG': 3431, 'GTTCTA': 3432, 'GTTCTT': 3433, 'GTTCTC': 3434, 'GTTCTG': 3435, 'GTTCCA': 3436, 'GTTCCT': 3437, 'GTTCCC': 3438, 'GTTCCG': 3439, 'GTTCGA': 3440, 'GTTCGT': 3441, 'GTTCGC': 3442, 'GTTCGG': 3443, 'GTTGAA': 3444, 'GTTGAT': 3445, 'GTTGAC': 3446, 'GTTGAG': 3447, 'GTTGTA': 3448, 'GTTGTT': 3449, 'GTTGTC': 3450, 'GTTGTG': 3451, 'GTTGCA': 3452, 'GTTGCT': 3453, 'GTTGCC': 3454, 'GTTGCG': 3455, 'GTTGGA': 3456, 'GTTGGT': 3457, 'GTTGGC': 3458, 'GTTGGG': 3459, 'GTCAAA': 3460, 'GTCAAT': 3461, 'GTCAAC': 3462, 'GTCAAG': 3463, 'GTCATA': 3464, 'GTCATT': 3465, 'GTCATC': 3466, 'GTCATG': 3467, 'GTCACA': 3468, 'GTCACT': 3469, 'GTCACC': 3470, 'GTCACG': 3471, 'GTCAGA': 3472, 'GTCAGT': 3473, 'GTCAGC': 3474, 'GTCAGG': 3475, 'GTCTAA': 3476, 'GTCTAT': 3477, 'GTCTAC': 3478, 'GTCTAG': 3479, 'GTCTTA': 3480, 'GTCTTT': 3481, 'GTCTTC': 3482, 'GTCTTG': 3483, 'GTCTCA': 3484, 'GTCTCT': 3485, 'GTCTCC': 3486, 'GTCTCG': 3487, 'GTCTGA': 3488, 'GTCTGT': 3489, 'GTCTGC': 3490, 'GTCTGG': 3491, 'GTCCAA': 3492, 'GTCCAT': 3493, 'GTCCAC': 3494, 'GTCCAG': 3495, 'GTCCTA': 3496, 'GTCCTT': 3497, 'GTCCTC': 3498, 'GTCCTG': 3499, 'GTCCCA': 3500, 'GTCCCT': 3501, 'GTCCCC': 3502, 'GTCCCG': 3503, 'GTCCGA': 3504, 'GTCCGT': 3505, 'GTCCGC': 3506, 'GTCCGG': 3507, 'GTCGAA': 3508, 'GTCGAT': 3509, 'GTCGAC': 3510, 'GTCGAG': 3511, 'GTCGTA': 3512, 'GTCGTT': 3513, 'GTCGTC': 3514, 'GTCGTG': 3515, 'GTCGCA': 3516, 'GTCGCT': 3517, 'GTCGCC': 3518, 'GTCGCG': 3519, 'GTCGGA': 3520, 'GTCGGT': 3521, 'GTCGGC': 3522, 'GTCGGG': 3523, 'GTGAAA': 3524, 'GTGAAT': 3525, 'GTGAAC': 3526, 'GTGAAG': 3527, 'GTGATA': 3528, 'GTGATT': 3529, 'GTGATC': 3530, 'GTGATG': 3531, 'GTGACA': 3532, 'GTGACT': 3533, 'GTGACC': 3534, 'GTGACG': 3535, 'GTGAGA': 3536, 'GTGAGT': 3537, 'GTGAGC': 3538, 'GTGAGG': 3539, 'GTGTAA': 3540, 'GTGTAT': 3541, 'GTGTAC': 3542, 'GTGTAG': 3543, 'GTGTTA': 3544, 'GTGTTT': 3545, 'GTGTTC': 3546, 'GTGTTG': 3547, 'GTGTCA': 3548, 'GTGTCT': 3549, 'GTGTCC': 3550, 'GTGTCG': 3551, 'GTGTGA': 3552, 'GTGTGT': 3553, 'GTGTGC': 3554, 'GTGTGG': 3555, 'GTGCAA': 3556, 'GTGCAT': 3557, 'GTGCAC': 3558, 'GTGCAG': 3559, 'GTGCTA': 3560, 'GTGCTT': 3561, 'GTGCTC': 3562, 'GTGCTG': 3563, 'GTGCCA': 3564, 'GTGCCT': 3565, 'GTGCCC': 3566, 'GTGCCG': 3567, 'GTGCGA': 3568, 'GTGCGT': 3569, 'GTGCGC': 3570, 'GTGCGG': 3571, 'GTGGAA': 3572, 'GTGGAT': 3573, 'GTGGAC': 3574, 'GTGGAG': 3575, 'GTGGTA': 3576, 'GTGGTT': 3577, 'GTGGTC': 3578, 'GTGGTG': 3579, 'GTGGCA': 3580, 'GTGGCT': 3581, 'GTGGCC': 3582, 'GTGGCG': 3583, 'GTGGGA': 3584, 'GTGGGT': 3585, 'GTGGGC': 3586, 'GTGGGG': 3587, 'GCAAAA': 3588, 'GCAAAT': 3589, 'GCAAAC': 3590, 'GCAAAG': 3591, 'GCAATA': 3592, 'GCAATT': 3593, 'GCAATC': 3594, 'GCAATG': 3595, 'GCAACA': 3596, 'GCAACT': 3597, 'GCAACC': 3598, 'GCAACG': 3599, 'GCAAGA': 3600, 'GCAAGT': 3601, 'GCAAGC': 3602, 'GCAAGG': 3603, 'GCATAA': 3604, 'GCATAT': 3605, 'GCATAC': 3606, 'GCATAG': 3607, 'GCATTA': 3608, 'GCATTT': 3609, 'GCATTC': 3610, 'GCATTG': 3611, 'GCATCA': 3612, 'GCATCT': 3613, 'GCATCC': 3614, 'GCATCG': 3615, 'GCATGA': 3616, 'GCATGT': 3617, 'GCATGC': 3618, 'GCATGG': 3619, 'GCACAA': 3620, 'GCACAT': 3621, 'GCACAC': 3622, 'GCACAG': 3623, 'GCACTA': 3624, 'GCACTT': 3625, 'GCACTC': 3626, 'GCACTG': 3627, 'GCACCA': 3628, 'GCACCT': 3629, 'GCACCC': 3630, 'GCACCG': 3631, 'GCACGA': 3632, 'GCACGT': 3633, 'GCACGC': 3634, 'GCACGG': 3635, 'GCAGAA': 3636, 'GCAGAT': 3637, 'GCAGAC': 3638, 'GCAGAG': 3639, 'GCAGTA': 3640, 'GCAGTT': 3641, 'GCAGTC': 3642, 'GCAGTG': 3643, 'GCAGCA': 3644, 'GCAGCT': 3645, 'GCAGCC': 3646, 'GCAGCG': 3647, 'GCAGGA': 3648, 'GCAGGT': 3649, 'GCAGGC': 3650, 'GCAGGG': 3651, 'GCTAAA': 3652, 'GCTAAT': 3653, 'GCTAAC': 3654, 'GCTAAG': 3655, 'GCTATA': 3656, 'GCTATT': 3657, 'GCTATC': 3658, 'GCTATG': 3659, 'GCTACA': 3660, 'GCTACT': 3661, 'GCTACC': 3662, 'GCTACG': 3663, 'GCTAGA': 3664, 'GCTAGT': 3665, 'GCTAGC': 3666, 'GCTAGG': 3667, 'GCTTAA': 3668, 'GCTTAT': 3669, 'GCTTAC': 3670, 'GCTTAG': 3671, 'GCTTTA': 3672, 'GCTTTT': 3673, 'GCTTTC': 3674, 'GCTTTG': 3675, 'GCTTCA': 3676, 'GCTTCT': 3677, 'GCTTCC': 3678, 'GCTTCG': 3679, 'GCTTGA': 3680, 'GCTTGT': 3681, 'GCTTGC': 3682, 'GCTTGG': 3683, 'GCTCAA': 3684, 'GCTCAT': 3685, 'GCTCAC': 3686, 'GCTCAG': 3687, 'GCTCTA': 3688, 'GCTCTT': 3689, 'GCTCTC': 3690, 'GCTCTG': 3691, 'GCTCCA': 3692, 'GCTCCT': 3693, 'GCTCCC': 3694, 'GCTCCG': 3695, 'GCTCGA': 3696, 'GCTCGT': 3697, 'GCTCGC': 3698, 'GCTCGG': 3699, 'GCTGAA': 3700, 'GCTGAT': 3701, 'GCTGAC': 3702, 'GCTGAG': 3703, 'GCTGTA': 3704, 'GCTGTT': 3705, 'GCTGTC': 3706, 'GCTGTG': 3707, 'GCTGCA': 3708, 'GCTGCT': 3709, 'GCTGCC': 3710, 'GCTGCG': 3711, 'GCTGGA': 3712, 'GCTGGT': 3713, 'GCTGGC': 3714, 'GCTGGG': 3715, 'GCCAAA': 3716, 'GCCAAT': 3717, 'GCCAAC': 3718, 'GCCAAG': 3719, 'GCCATA': 3720, 'GCCATT': 3721, 'GCCATC': 3722, 'GCCATG': 3723, 'GCCACA': 3724, 'GCCACT': 3725, 'GCCACC': 3726, 'GCCACG': 3727, 'GCCAGA': 3728, 'GCCAGT': 3729, 'GCCAGC': 3730, 'GCCAGG': 3731, 'GCCTAA': 3732, 'GCCTAT': 3733, 'GCCTAC': 3734, 'GCCTAG': 3735, 'GCCTTA': 3736, 'GCCTTT': 3737, 'GCCTTC': 3738, 'GCCTTG': 3739, 'GCCTCA': 3740, 'GCCTCT': 3741, 'GCCTCC': 3742, 'GCCTCG': 3743, 'GCCTGA': 3744, 'GCCTGT': 3745, 'GCCTGC': 3746, 'GCCTGG': 3747, 'GCCCAA': 3748, 'GCCCAT': 3749, 'GCCCAC': 3750, 'GCCCAG': 3751, 'GCCCTA': 3752, 'GCCCTT': 3753, 'GCCCTC': 3754, 'GCCCTG': 3755, 'GCCCCA': 3756, 'GCCCCT': 3757, 'GCCCCC': 3758, 'GCCCCG': 3759, 'GCCCGA': 3760, 'GCCCGT': 3761, 'GCCCGC': 3762, 'GCCCGG': 3763, 'GCCGAA': 3764, 'GCCGAT': 3765, 'GCCGAC': 3766, 'GCCGAG': 3767, 'GCCGTA': 3768, 'GCCGTT': 3769, 'GCCGTC': 3770, 'GCCGTG': 3771, 'GCCGCA': 3772, 'GCCGCT': 3773, 'GCCGCC': 3774, 'GCCGCG': 3775, 'GCCGGA': 3776, 'GCCGGT': 3777, 'GCCGGC': 3778, 'GCCGGG': 3779, 'GCGAAA': 3780, 'GCGAAT': 3781, 'GCGAAC': 3782, 'GCGAAG': 3783, 'GCGATA': 3784, 'GCGATT': 3785, 'GCGATC': 3786, 'GCGATG': 3787, 'GCGACA': 3788, 'GCGACT': 3789, 'GCGACC': 3790, 'GCGACG': 3791, 'GCGAGA': 3792, 'GCGAGT': 3793, 'GCGAGC': 3794, 'GCGAGG': 3795, 'GCGTAA': 3796, 'GCGTAT': 3797, 'GCGTAC': 3798, 'GCGTAG': 3799, 'GCGTTA': 3800, 'GCGTTT': 3801, 'GCGTTC': 3802, 'GCGTTG': 3803, 'GCGTCA': 3804, 'GCGTCT': 3805, 'GCGTCC': 3806, 'GCGTCG': 3807, 'GCGTGA': 3808, 'GCGTGT': 3809, 'GCGTGC': 3810, 'GCGTGG': 3811, 'GCGCAA': 3812, 'GCGCAT': 3813, 'GCGCAC': 3814, 'GCGCAG': 3815, 'GCGCTA': 3816, 'GCGCTT': 3817, 'GCGCTC': 3818, 'GCGCTG': 3819, 'GCGCCA': 3820, 'GCGCCT': 3821, 'GCGCCC': 3822, 'GCGCCG': 3823, 'GCGCGA': 3824, 'GCGCGT': 3825, 'GCGCGC': 3826, 'GCGCGG': 3827, 'GCGGAA': 3828, 'GCGGAT': 3829, 'GCGGAC': 3830, 'GCGGAG': 3831, 'GCGGTA': 3832, 'GCGGTT': 3833, 'GCGGTC': 3834, 'GCGGTG': 3835, 'GCGGCA': 3836, 'GCGGCT': 3837, 'GCGGCC': 3838, 'GCGGCG': 3839, 'GCGGGA': 3840, 'GCGGGT': 3841, 'GCGGGC': 3842, 'GCGGGG': 3843, 'GGAAAA': 3844, 'GGAAAT': 3845, 'GGAAAC': 3846, 'GGAAAG': 3847, 'GGAATA': 3848, 'GGAATT': 3849, 'GGAATC': 3850, 'GGAATG': 3851, 'GGAACA': 3852, 'GGAACT': 3853, 'GGAACC': 3854, 'GGAACG': 3855, 'GGAAGA': 3856, 'GGAAGT': 3857, 'GGAAGC': 3858, 'GGAAGG': 3859, 'GGATAA': 3860, 'GGATAT': 3861, 'GGATAC': 3862, 'GGATAG': 3863, 'GGATTA': 3864, 'GGATTT': 3865, 'GGATTC': 3866, 'GGATTG': 3867, 'GGATCA': 3868, 'GGATCT': 3869, 'GGATCC': 3870, 'GGATCG': 3871, 'GGATGA': 3872, 'GGATGT': 3873, 'GGATGC': 3874, 'GGATGG': 3875, 'GGACAA': 3876, 'GGACAT': 3877, 'GGACAC': 3878, 'GGACAG': 3879, 'GGACTA': 3880, 'GGACTT': 3881, 'GGACTC': 3882, 'GGACTG': 3883, 'GGACCA': 3884, 'GGACCT': 3885, 'GGACCC': 3886, 'GGACCG': 3887, 'GGACGA': 3888, 'GGACGT': 3889, 'GGACGC': 3890, 'GGACGG': 3891, 'GGAGAA': 3892, 'GGAGAT': 3893, 'GGAGAC': 3894, 'GGAGAG': 3895, 'GGAGTA': 3896, 'GGAGTT': 3897, 'GGAGTC': 3898, 'GGAGTG': 3899, 'GGAGCA': 3900, 'GGAGCT': 3901, 'GGAGCC': 3902, 'GGAGCG': 3903, 'GGAGGA': 3904, 'GGAGGT': 3905, 'GGAGGC': 3906, 'GGAGGG': 3907, 'GGTAAA': 3908, 'GGTAAT': 3909, 'GGTAAC': 3910, 'GGTAAG': 3911, 'GGTATA': 3912, 'GGTATT': 3913, 'GGTATC': 3914, 'GGTATG': 3915, 'GGTACA': 3916, 'GGTACT': 3917, 'GGTACC': 3918, 'GGTACG': 3919, 'GGTAGA': 3920, 'GGTAGT': 3921, 'GGTAGC': 3922, 'GGTAGG': 3923, 'GGTTAA': 3924, 'GGTTAT': 3925, 'GGTTAC': 3926, 'GGTTAG': 3927, 'GGTTTA': 3928, 'GGTTTT': 3929, 'GGTTTC': 3930, 'GGTTTG': 3931, 'GGTTCA': 3932, 'GGTTCT': 3933, 'GGTTCC': 3934, 'GGTTCG': 3935, 'GGTTGA': 3936, 'GGTTGT': 3937, 'GGTTGC': 3938, 'GGTTGG': 3939, 'GGTCAA': 3940, 'GGTCAT': 3941, 'GGTCAC': 3942, 'GGTCAG': 3943, 'GGTCTA': 3944, 'GGTCTT': 3945, 'GGTCTC': 3946, 'GGTCTG': 3947, 'GGTCCA': 3948, 'GGTCCT': 3949, 'GGTCCC': 3950, 'GGTCCG': 3951, 'GGTCGA': 3952, 'GGTCGT': 3953, 'GGTCGC': 3954, 'GGTCGG': 3955, 'GGTGAA': 3956, 'GGTGAT': 3957, 'GGTGAC': 3958, 'GGTGAG': 3959, 'GGTGTA': 3960, 'GGTGTT': 3961, 'GGTGTC': 3962, 'GGTGTG': 3963, 'GGTGCA': 3964, 'GGTGCT': 3965, 'GGTGCC': 3966, 'GGTGCG': 3967, 'GGTGGA': 3968, 'GGTGGT': 3969, 'GGTGGC': 3970, 'GGTGGG': 3971, 'GGCAAA': 3972, 'GGCAAT': 3973, 'GGCAAC': 3974, 'GGCAAG': 3975, 'GGCATA': 3976, 'GGCATT': 3977, 'GGCATC': 3978, 'GGCATG': 3979, 'GGCACA': 3980, 'GGCACT': 3981, 'GGCACC': 3982, 'GGCACG': 3983, 'GGCAGA': 3984, 'GGCAGT': 3985, 'GGCAGC': 3986, 'GGCAGG': 3987, 'GGCTAA': 3988, 'GGCTAT': 3989, 'GGCTAC': 3990, 'GGCTAG': 3991, 'GGCTTA': 3992, 'GGCTTT': 3993, 'GGCTTC': 3994, 'GGCTTG': 3995, 'GGCTCA': 3996, 'GGCTCT': 3997, 'GGCTCC': 3998, 'GGCTCG': 3999, 'GGCTGA': 4000, 'GGCTGT': 4001, 'GGCTGC': 4002, 'GGCTGG': 4003, 'GGCCAA': 4004, 'GGCCAT': 4005, 'GGCCAC': 4006, 'GGCCAG': 4007, 'GGCCTA': 4008, 'GGCCTT': 4009, 'GGCCTC': 4010, 'GGCCTG': 4011, 'GGCCCA': 4012, 'GGCCCT': 4013, 'GGCCCC': 4014, 'GGCCCG': 4015, 'GGCCGA': 4016, 'GGCCGT': 4017, 'GGCCGC': 4018, 'GGCCGG': 4019, 'GGCGAA': 4020, 'GGCGAT': 4021, 'GGCGAC': 4022, 'GGCGAG': 4023, 'GGCGTA': 4024, 'GGCGTT': 4025, 'GGCGTC': 4026, 'GGCGTG': 4027, 'GGCGCA': 4028, 'GGCGCT': 4029, 'GGCGCC': 4030, 'GGCGCG': 4031, 'GGCGGA': 4032, 'GGCGGT': 4033, 'GGCGGC': 4034, 'GGCGGG': 4035, 'GGGAAA': 4036, 'GGGAAT': 4037, 'GGGAAC': 4038, 'GGGAAG': 4039, 'GGGATA': 4040, 'GGGATT': 4041, 'GGGATC': 4042, 'GGGATG': 4043, 'GGGACA': 4044, 'GGGACT': 4045, 'GGGACC': 4046, 'GGGACG': 4047, 'GGGAGA': 4048, 'GGGAGT': 4049, 'GGGAGC': 4050, 'GGGAGG': 4051, 'GGGTAA': 4052, 'GGGTAT': 4053, 'GGGTAC': 4054, 'GGGTAG': 4055, 'GGGTTA': 4056, 'GGGTTT': 4057, 'GGGTTC': 4058, 'GGGTTG': 4059, 'GGGTCA': 4060, 'GGGTCT': 4061, 'GGGTCC': 4062, 'GGGTCG': 4063, 'GGGTGA': 4064, 'GGGTGT': 4065, 'GGGTGC': 4066, 'GGGTGG': 4067, 'GGGCAA': 4068, 'GGGCAT': 4069, 'GGGCAC': 4070, 'GGGCAG': 4071, 'GGGCTA': 4072, 'GGGCTT': 4073, 'GGGCTC': 4074, 'GGGCTG': 4075, 'GGGCCA': 4076, 'GGGCCT': 4077, 'GGGCCC': 4078, 'GGGCCG': 4079, 'GGGCGA': 4080, 'GGGCGT': 4081, 'GGGCGC': 4082, 'GGGCGG': 4083, 'GGGGAA': 4084, 'GGGGAT': 4085, 'GGGGAC': 4086, 'GGGGAG': 4087, 'GGGGTA': 4088, 'GGGGTT': 4089, 'GGGGTC': 4090, 'GGGGTG': 4091, 'GGGGCA': 4092, 'GGGGCT': 4093, 'GGGGCC': 4094, 'GGGGCG': 4095, 'GGGGGA': 4096, 'GGGGGT': 4097, 'GGGGGC': 4098, 'GGGGGG': 4099, 'A': 4100, 'T': 4101, 'C': 4102, 'G': 4103, 'N': 4104, '<eos>': 4105, '<bos>': 4106}\n",
      "{'[PAD]': 0, 'a': 3, 'g': 5, 'c': 4, '[MASK]': 1, '[UNK]': 2, 't': 6}\n"
     ]
    }
   ],
   "source": [
    "def create_alphabet_dict(file_path):\n",
    "    alphabet = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for index, line in enumerate(file):\n",
    "            token = line.strip()  # Remove any trailing newline characters\n",
    "            alphabet[token] = index\n",
    "    return alphabet\n",
    "\n",
    "# Usage\n",
    "kmer_file_path = '/common/zhangz2lab/zhanh/dnabert-config/bert-config-6/vocab_nt.txt'  # Replace with your file path\n",
    "one_hot_file_path = 'common/zhangz2lab/zhanh/dnabert-config/bert-config-6/one_hot.txt'# Replace with your file path\n",
    "cm_path = '/common/zhangz2lab/zhanh/Jupyter_Scripts/cm_spair/test_200.csv'# Replace with your file path\n",
    "\n",
    "alphabet_dict = create_alphabet_dict(kmer_file_path)\n",
    "print(alphabet_dict)\n",
    "alphabet_dict = {'[PAD]': 0, 'a': 3, 'g': 5, 'c': 4, '[MASK]': 1, '[UNK]': 2, 't': 6}\n",
    "print(alphabet_dict)\n",
    "alphabet_dict = {'[PAD]': 0, 'A': 3, 'G': 5, 'C': 4, '[MASK]': 1, '[UNK]': 2, 'T': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a1bc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerControl\n",
    "loss_name =\"contrastive+BCE\"\n",
    "\n",
    "class TestSetCallback(TrainerCallback):\n",
    "    def __init__(self, model, trainer, test_dataset, eval_steps, tokenizer):\n",
    "        self.model = model\n",
    "        self.trainer = trainer\n",
    "        self.test_dataset = test_dataset\n",
    "        self.eval_steps = eval_steps\n",
    "        self.tokenizer = tokenizer\n",
    "        self.step_count = 0\n",
    "        self.alphabet = alphabet_dict\n",
    "        \n",
    "    def plot_roc(self, labels, logits):\n",
    "        fpr, tpr, _ = roc_curve(labels, logits)\n",
    "        roc_curve_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_curve_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        \n",
    "                \n",
    "    def compute_pll_for_sequence(self, sequence, model):\n",
    "        #tokens = self.tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        tokens = self.tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "        model_device = next(model.parameters()).device\n",
    "        for key in tokens.keys():\n",
    "            tokens[key] = tokens[key].to(model_device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model.base_model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "        \n",
    "        logits = torch.log_softmax(outputs.logits, dim=-1)\n",
    "        #print('logits',logits)\n",
    "        idx = [self.alphabet[t] for t in sequence]\n",
    "        PLL = torch.sum(torch.diag(logits[0, 1:-1, :][:, idx]))\n",
    "        return PLL.item(), logits\n",
    "        \n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self.step_count += 1\n",
    "        lambda_training_steps = self.model.lambda_training_steps\n",
    "        self.model.lambda_weight = min(1, self.step_count / lambda_training_steps)\n",
    "        self.model.lambda_weight = 0.05\n",
    "        if state.global_step % 2000 == 0:  # Adjust interval as needed\n",
    "            #self.model.eval() \n",
    "            print(f\"Step {state.global_step}, Lambda: {self.model.lambda_weight}\")\n",
    "            ##test_results = self.trainer.evaluate(self.test_dataset)\n",
    "            #self.model.train()\n",
    "            ##print(test_results)\n",
    "            \n",
    "\n",
    "        \n",
    "        #if self.step_count == 2 or state.global_step % self.eval_steps == 0:\n",
    "        if (self.step_count == 2 or (self.step_count > 2 and state.global_step % self.eval_steps == 0)) and loss_name ==\"PLLR\":\n",
    "            # Perform evaluation and plot ROC\n",
    "            predictions = self.trainer.predict(self.test_dataset)\n",
    "            logits = predictions.predictions\n",
    "            labels = predictions.label_ids\n",
    "\n",
    "\n",
    "            self.plot_roc(labels, logits)\n",
    "            \n",
    "            all_sequences = []\n",
    "            df = pd.read_csv(cm_path)\n",
    "            all_sequences = df['seq_a'].tolist()\n",
    "\n",
    "            all_plls_wt = []\n",
    "            all_plls_wt_weighted = []\n",
    "            \n",
    "            for seq in all_sequences:\n",
    "                wt_pll, wt_logits = self.compute_pll_for_sequence(seq, model)\n",
    "                print(wt_pll)\n",
    "                all_plls_wt.append(wt_pll)\n",
    "                all_plls_wt_weighted.append(wt_pll / len(seq))\n",
    "                \n",
    "            \n",
    "            all_sequences = []\n",
    "            all_sequences = df['seq_b'].tolist()\n",
    "\n",
    "            all_plls_mut = []\n",
    "            all_plls_mut_weighted = []\n",
    "            for seq in all_sequences:\n",
    "                mut_pll, _ = self.compute_pll_for_sequence(seq, model)\n",
    "                print(mut_pll)\n",
    "                all_plls_mut.append(mut_pll)\n",
    "                all_plls_mut_weighted.append(mut_pll / len(seq))\n",
    "\n",
    "            all_plls_wt = np.array(all_plls_wt)\n",
    "            all_plls_mut = np.array(all_plls_mut)\n",
    "            \n",
    "            all_plls_wt_weighted = np.array(all_plls_wt_weighted)\n",
    "            all_plls_mut_weighted = np.array(all_plls_mut_weighted)\n",
    "        \n",
    "        # Compute the PLLR\n",
    "            PLLR_callback = np.abs(all_plls_wt - all_plls_mut)\n",
    "            PLLR_weighted_callback = np.abs(all_plls_wt_weighted - all_plls_mut_weighted)\n",
    "        \n",
    "        # Get true labels\n",
    "            true_labels_callback = df['labels'].to_numpy()\n",
    "            fpr, tpr, _ = roc_curve(true_labels_callback, PLLR_callback)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aupr = average_precision_score(true_labels_callback, PLLR_callback)\n",
    "\n",
    "            # Compute metrics for PLLR_weighted_callback\n",
    "            fpr_weighted, tpr_weighted, _ = roc_curve(true_labels_callback, PLLR_weighted_callback)\n",
    "            roc_auc_weighted = auc(fpr_weighted, tpr_weighted)\n",
    "            aupr_weighted = average_precision_score(true_labels_callback, PLLR_weighted_callback)\n",
    "\n",
    "            # Plotting ROC for both PLLR_callback and PLLR_weighted_callback\n",
    "            #plt.figure()\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            mpl.rcParams['font.size'] = 18\n",
    "            lw = 2  # line width\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=lw, label='PLLR ROC curve (area = %0.2f)' % roc_auc)\n",
    "            plt.plot(fpr_weighted, tpr_weighted, color='darkgreen', lw=lw, label='weighted PLLR ROC curve (area = %0.2f)' % roc_auc_weighted)\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) for PLLR and weighted PLLR')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "# class CustomCallback(TrainerCallback):\n",
    "#     def __init__(self, tokenizer):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.step_count = 0\n",
    "#         self.alphabet = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
    "\n",
    "#     def compute_pll_for_sequence(self, sequence, model):\n",
    "#         #tokens = self.tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#         tokens = self.tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=training_args.model_max_length)\n",
    "#         model_device = next(model.parameters()).device\n",
    "#         for key in tokens.keys():\n",
    "#             tokens[key] = tokens[key].to(model_device)\n",
    "            \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.base_model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "        \n",
    "#         logits = torch.log_softmax(outputs.logits, dim=-1)\n",
    "#         #print('logits',logits)\n",
    "#         idx = [self.alphabet[t] for t in sequence]\n",
    "#         PLL = torch.sum(torch.diag(logits[0, 1:-1, :][:, idx]))\n",
    "#         return PLL.item(), logits\n",
    "    \n",
    "\n",
    "#     def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "#         self.step_count += 1\n",
    "#         if self.step_count == 1 or self.step_count % 50 == 0:  # You can adjust the frequency as needed.\n",
    "#             all_sequences = []\n",
    "#             df = pd.read_csv(\"/common/zhangz2lab/zhanh/esm-variants/cropped/cm_test_data_1024.csv\")\n",
    "#             all_sequences = df['wt_seq'].tolist()\n",
    "#             df_arm = pd.read_csv(\"/common/zhangz2lab/zhanh/esm-variants/cropped/arm_test_data_1024.csv\")\n",
    "#             sequence_arm = df_arm['wt_seq'].iloc[0]\n",
    "\n",
    "#             all_plls_wt = []\n",
    "#             all_plls_wt_weighted = []\n",
    "            \n",
    "#             for seq in all_sequences:\n",
    "#                 wt_pll, wt_logits = self.compute_pll_for_sequence(seq, model)\n",
    "#                 all_plls_wt.append(wt_pll)\n",
    "#                 all_plls_wt_weighted.append(wt_pll / len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61853275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Definition\n",
    "# class SiameseDataset(Dataset):\n",
    "#     def __init__(self, tokenizer, filename):\n",
    "#         data = pd.read_csv(filename)\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.seq_a = list(data['seq_a'])\n",
    "#         self.seq_b = list(data['seq_b'])\n",
    "#         self.labels = list(data['labels'])\n",
    "#         self.num_examples = len(self.labels)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.num_examples\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         inputs_a = self.tokenizer(self.seq_a[idx], return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=512)\n",
    "#         inputs_b = self.tokenizer(self.seq_b[idx], return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=512)\n",
    "#         return {\n",
    "#             \"input_ids1\": inputs_a[\"input_ids\"].squeeze(0), \n",
    "#             \"attention_mask1\": inputs_a[\"attention_mask\"].squeeze(0),\n",
    "#             \"input_ids2\": inputs_b[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask2\": inputs_b[\"attention_mask\"].squeeze(0),\n",
    "#             \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         }\n",
    "    \n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, tokenizer_type, filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.seq_a = list(data['seq_a'])\n",
    "        self.seq_b = list(data['seq_b'])\n",
    "        self.labels = list(data['labels'])\n",
    "        self.num_examples = len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.tokenizer_type == \"bpe\":\n",
    "            inputs_a = self.tokenizer(self.seq_a[idx], return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=512)\n",
    "            inputs_b = self.tokenizer(self.seq_b[idx], return_tensors=\"pt\", truncation=True, padding=\"longest\", max_length=512)\n",
    "            return {\n",
    "                \"input_ids1\": inputs_a[\"input_ids\"].squeeze(0), \n",
    "                \"attention_mask1\": inputs_a[\"attention_mask\"].squeeze(0),\n",
    "                \"input_ids2\": inputs_b[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask2\": inputs_b[\"attention_mask\"].squeeze(0),\n",
    "                \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            }\n",
    "        elif self.tokenizer_type == \"6-mer\" or self.tokenizer_type == \"one-hot\":  # Assume the other type is the custom overlapping tokenizer\n",
    "            #print(\"Using 6-meer\")\n",
    "            input_ids_a = torch.tensor(self.tokenizer(self.seq_a[idx]))\n",
    "            input_ids_b = torch.tensor(self.tokenizer(self.seq_b[idx]))\n",
    "            \n",
    "            attention_mask_a = torch.ones_like(input_ids_a)\n",
    "            attention_mask_b = torch.ones_like(input_ids_b)\n",
    "            max_length = 512\n",
    "            if len(input_ids_a) < max_length:\n",
    "                padding_size = max_length - len(input_ids_a)\n",
    "                input_ids_a = F.pad(input_ids_a, pad=(0, padding_size), value=0)\n",
    "                attention_mask_a = F.pad(attention_mask_a, pad=(0, padding_size), value=0)\n",
    "            \n",
    "            if len(input_ids_b) < max_length:\n",
    "                padding_size = max_length - len(input_ids_b)\n",
    "                input_ids_b = F.pad(input_ids_b, pad=(0, padding_size), value=0)\n",
    "                attention_mask_b = F.pad(attention_mask_b, pad=(0, padding_size), value=0)\n",
    "\n",
    "            return {\n",
    "                \"input_ids1\": input_ids_a.squeeze(0),\n",
    "                \"attention_mask1\": attention_mask_a.squeeze(0),\n",
    "                \"input_ids2\": input_ids_b.squeeze(0),\n",
    "                \"attention_mask2\": attention_mask_b.squeeze(0),\n",
    "                \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94662245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self, model_name_or_path, num_labels, cache_dir=None):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "\n",
    "#         # Load the base model\n",
    "#         self.base_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "#             model_name_or_path,\n",
    "#             cache_dir=cache_dir,\n",
    "#             num_labels=num_labels,\n",
    "#             trust_remote_code=True,\n",
    "#             output_hidden_states=True\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#     def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, labels):\n",
    "#         # Encoding sequences using the same base model\n",
    "#         logits1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1).logits\n",
    "#         logits2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2).logits\n",
    "#         #pooler_output1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1).pooler_output\n",
    "#         #pooler_output2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2).pooler_output\n",
    "#         outputs1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "#         outputs2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "#         last_hidden_state1 = outputs1.hidden_states[-1][:, 0, :]\n",
    "#         last_hidden_state2 = outputs2.hidden_states[-1][:, 0, :]\n",
    "#         #output1 = logits1[:, 0]\n",
    "#         #output2 = logits2[:, 0]\n",
    "#         #cosine_sim = F.cosine_similarity(output1, output2, dim=-1)\n",
    "#         cosine_sim = F.cosine_similarity(last_hidden_state1, last_hidden_state2, dim=-1)\n",
    "#         mapped_sim = (cosine_sim + 1) / 2\n",
    "#         loss = F.mse_loss(mapped_sim, labels.float())\n",
    "        \n",
    "#         return (loss, cosine_sim)\n",
    "\n",
    "    \n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model_name_or_path, num_labels, loss_type=\"contrastive+BCE\", margin=2.0, lambda_weight=0.5, cache_dir=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "        self.margin = margin\n",
    "        self.lambda_weight = lambda_weight\n",
    "        self.lambda_training_steps = 6000 \n",
    "\n",
    "        # Load the base model\n",
    "        self.base_model = transformers.AutoModel.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            cache_dir=cache_dir,\n",
    "            num_labels=num_labels,\n",
    "            trust_remote_code=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Freeze the parameters of the base model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "             nn.Linear(self.base_model.config.hidden_size * 2, 128),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, labels):\n",
    "        # Encoding sequences using the same base model\n",
    "        outputs1 = self.base_model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "        outputs2 = self.base_model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "        #print(\"Shape of logits1:\", outputs1.logits.shape)\n",
    "\n",
    "        #last_hidden_state1 = outputs1.hidden_states[-1][:, 0, :]\n",
    "        #last_hidden_state2 = outputs2.hidden_states[-1][:, 0, :]\n",
    "        last_hidden_state1 = outputs1.last_hidden_state[:, 0, :]\n",
    "        last_hidden_state2 = outputs2.last_hidden_state[:, 0, :]\n",
    "        #print(\"Shape of last hidden state 1:\", last_hidden_state1.shape)\n",
    "        if self.loss_type == \"PLLR\":\n",
    "            logits1 = torch.log_softmax(outputs1.logits, dim=-1)\n",
    "            logits2 = torch.log_softmax(outputs2.logits, dim=-1)\n",
    "            #print('logits1',logits1)\n",
    "            #print('logits2',logits2)\n",
    "            batch_size = input_ids1.shape[0]\n",
    "        \n",
    "        \n",
    "            PLLs1 = torch.zeros(batch_size, device=input_ids1.device)\n",
    "            PLLs2 = torch.zeros(batch_size, device=input_ids2.device)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                idx1 = input_ids1[i, 1:-1]  # Excluding the special tokens <cls> and <eos>/<pad>\n",
    "                PLLs1[i] = torch.sum(torch.diag(logits1[i, 1:-1, :][:, idx1]))\n",
    "            for i in range(batch_size):\n",
    "                idx2 = input_ids2[i, 1:-1]  # Excluding the special tokens <cls> and <eos>/<pad>\n",
    "                PLLs2[i] = torch.sum(torch.diag(logits2[i, 1:-1, :][:, idx2]))\n",
    "            PLLR = torch.abs(PLLs1 - PLLs2)\n",
    "            print(PLLR)\n",
    "            PLLR_t = PLLs1 - PLLs2\n",
    "            print(PLLR_t)\n",
    "            sigmoid_PLLR = torch.sigmoid(PLLR)\n",
    "            pll_loss = F.binary_cross_entropy(2*sigmoid_PLLR-1, labels.float())\n",
    "\n",
    "            #return (loss, cosine_sim)\n",
    "            return (pll_loss, PLLR)\n",
    "\n",
    "        elif self.loss_type == \"cosine\":\n",
    "            cosine_sim = F.cosine_similarity(last_hidden_state1, last_hidden_state2, dim=-1)\n",
    "            mapped_sim = (cosine_sim + 1) / 2  # map between 0 and 1\n",
    "            # Invert mapped_sim when the label is 1 to encourage dissimilarity\n",
    "            inverted_mapped_sim = 1 - mapped_sim\n",
    "            # Use label to choose between mapped_sim and inverted_mapped_sim\n",
    "            adjusted_sim = labels.float() * inverted_mapped_sim + (1 - labels.float()) * mapped_sim\n",
    "            loss = F.mse_loss(adjusted_sim, labels.float())\n",
    "            return (loss, cosine_sim)\n",
    "        \n",
    "        elif self.loss_type == \"contrastive\":\n",
    "            euclidean_distance = F.pairwise_distance(last_hidden_state1, last_hidden_state2)\n",
    "            # Contrastive loss\n",
    "            loss = (1 - labels) * torch.pow(euclidean_distance, 2) + \\\n",
    "                   labels * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "            loss = torch.mean(loss)\n",
    "            return (loss, euclidean_distance)\n",
    "        \n",
    "        elif self.loss_type == \"contrastive+BCE\":\n",
    "            euclidean_distance = F.pairwise_distance(last_hidden_state1, last_hidden_state2)\n",
    "            # Contrastive loss\n",
    "            contrastive_loss = (1 - labels) * torch.pow(euclidean_distance, 2) + \\\n",
    "                   labels * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "            contrastive_loss = torch.mean(contrastive_loss)\n",
    "            # Binary Cross-Entropy (BCE) Loss\n",
    "            outputs = self.classifier(torch.cat([last_hidden_state1, last_hidden_state2], dim=1))\n",
    "            bce_loss = F.cross_entropy(outputs, labels)\n",
    "            #self.lambda_weight = min(1, self.forward_count /self.lambda_training_steps)\n",
    "            # Combined loss\n",
    "            total_loss = contrastive_loss + self.lambda_weight * bce_loss\n",
    "            return (total_loss, outputs, euclidean_distance)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss_type specified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dfb1b92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#      #optim = \"adamw_torch\",\n",
    "#      run_name = \"run\",\n",
    "#      gradient_accumulation_steps =1,\n",
    "#      per_device_train_batch_size=8,\n",
    "#      per_device_eval_batch_size=8,\n",
    "#      fp16=False,\n",
    "#      num_train_epochs=30,\n",
    "#      evaluation_strategy=\"steps\",\n",
    "#      eval_steps = 100,\n",
    "#      save_steps=100,\n",
    "#      logging_dir='/common/zhangz2lab/zhanh/Jupyter_Scripts/output_0824/logs',\n",
    "#      logging_steps=100,\n",
    "#      load_best_model_at_end=True,  # this is defined twice in your class, consider removing one\n",
    "#      metric_for_best_model=\"eval_loss\",\n",
    "#      greater_is_better=False,\n",
    "#      #early_stopping_patience=3,  # Number of evaluations without improvement to wait before stopping training\n",
    "#      #early_stopping_threshold=0.001,\n",
    "#      logging_strategy=\"steps\",\n",
    "#      #warmup_ratio=0.1,\n",
    "#      weight_decay=1e-4,\n",
    "#      learning_rate=2e-5,\n",
    "#      #lr_scheduler_type='linear',\n",
    "#      do_train=True,\n",
    "#      do_eval=True,\n",
    "#      output_dir='/common/zhangz2lab/zhanh/Jupyter_Scripts/output_0825/results',\n",
    "#      save_strategy='steps',\n",
    "#      save_total_limit=5,\n",
    "#      push_to_hub=False,\n",
    "#      dataloader_pin_memory=False,\n",
    "#      seed=42,\n",
    "#      logging_first_step=True\n",
    "#  )\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#      per_device_train_batch_size=8,\n",
    "#      per_device_eval_batch_size=8,\n",
    "#      num_train_epochs=10,\n",
    "#      evaluation_strategy=\"steps\",\n",
    "#      logging_dir='/common/zhangz2lab/zhanh/Jupyter_Scripts/output_0824/logs',\n",
    "#      logging_steps=100,\n",
    "#      do_train=True,\n",
    "#      do_eval=True,\n",
    "#      output_dir='/common/zhangz2lab/zhanh/Jupyter_Scripts/output_0824/results',\n",
    "#      save_strategy='steps',\n",
    "#      save_total_limit=2,\n",
    "#      push_to_hub=False,\n",
    "#  )\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=8)\n",
    "    per_device_eval_batch_size: int = field(default=8)\n",
    "    num_train_epochs: int = field(default=5)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=1000)\n",
    "    save_steps: int = field(default=1000)\n",
    "    eval_steps: int = field(default=1000)\n",
    "    evaluation_strategy: str = field(default=\"steps\")\n",
    "    load_best_model_at_end: bool = field(default=True)     # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model: str = field(default=\"auc\") # the metric to use to compare models\n",
    "    greater_is_better: bool = field(default=True)           # whether the `metric_for_best_model` should be maximized or not\n",
    "    logging_strategy: str = field(default=\"steps\")  # Log every \"steps\"\n",
    "    #logging_steps: int = field(default=5000)  # Log every 100 steps\n",
    "    warmup_ratio: int = field(default=0.1)\n",
    "    weight_decay: float = field(default=1e-4)\n",
    "    learning_rate: float = field(default=2e-5)\n",
    "    lr_scheduler_type: str = field(default='linear')\n",
    "    save_total_limit: int = field(default=5)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"./results\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    early_stopping_patience: int = field(default = 5)  # number of evaluations without improvement to wait\n",
    "    early_stopping_threshold: float = field(default = 1e-3)  # threshold for an improvement\n",
    "        \n",
    "training_args = TrainingArguments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77f05fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name_or_path: Optional[str] = field(default=\"bert-base-uncased\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"InstaDeepAI/nucleotide-transformer-500m-human-ref\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"InstaDeepAI/nucleotide-transformer-2.5b-1000g\")\n",
    "    model_name_or_path: Optional[str] = field(default=\"songlab/gpn-brassicales\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"/common/zhangz2lab/zhanh/Jupyter_Scripts/output_0220/results_gpn/checkpoint-gpn-2\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"/common/zhangz2lab/zhanh/SpliceBERT/examples/models/SpliceBERT.1024nt\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"facebook/esm1b_t33_650M_UR50S\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"decapoda-research/llama-7b-hf\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "    #model_name_or_path: Optional[str] = field(default=\"zhihan1996/DNABERT-2-117M\")\n",
    "    use_lora: bool = field(default=False, metadata={\"help\": \"whether to use LoRA\"})\n",
    "    lora_r: int = field(default=8, metadata={\"help\": \"hidden dimension for LoRA\"})\n",
    "    lora_alpha: int = field(default=32, metadata={\"help\": \"alpha for LoRA\"})\n",
    "    lora_dropout: float = field(default=0.05, metadata={\"help\": \"dropout rate for LoRA\"})\n",
    "    #lora_target_modules: str = field(default=\"k_proj,q_proj,v_proj,fc1,fc2,output_proj\", metadata={\"help\": \"where to perform LoRA\"})\n",
    "    #lora_target_modules: str = field(default=\"Wqkv,dense,mlp.wo\", metadata={\"help\": \"where to perform LoRA\"})\n",
    "    lora_target_modules: str = field(default=\"query,key,value\", metadata={\"help\": \"where to perform LoRA\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253e42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a1a1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42c5dca2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at songlab/gpn-brassicales were not used when initializing ConvNetModel: ['cls.decoder.2.weight', 'cls.decoder.0.weight', 'cls.decoder.0.bias', 'cls.decoder.3.weight', 'cls.decoder.3.bias', 'cls.decoder.2.bias']\n",
      "- This IS expected if you are initializing ConvNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ConvNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bpe tokenizer\n",
      "{'input_ids': [3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 5, 3, 3, 3, 3, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#model_name_or_path = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "#data_path = \"/common/zhangz2lab/zhanh/Jupyter_Scripts/cm_spair/\"\n",
    "data_path = \"/common/zhangz2lab/zhanh/MFASS/processed_data/snv/\"\n",
    "test_data_path = \"/common/zhangz2lab/zhanh/Clinvar/\"\n",
    "#model_name_or_path = \"bert-base-uncased\"\n",
    "tokenizer_name = \"bpe\"\n",
    "model = SiameseNetwork(model_args.model_name_or_path, num_labels=2)\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(name, param.requires_grad)\n",
    "    \n",
    "if model_args.use_lora:\n",
    "        lora_config = LoraConfig(\n",
    "            r = model_args.lora_r,\n",
    "            #init_r = 12,\n",
    "            #target_r = 8,\n",
    "            #target_modules=list(r\"bert\\.encoder\\.layer\\.\\d+\\.mlp\\.wo\"),\n",
    "            lora_alpha=model_args.lora_alpha,\n",
    "            target_modules=list(model_args.lora_target_modules.split(\",\")),\n",
    "            #target_modules = target[1:],\n",
    "            lora_dropout=model_args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "            #peft_type=\"ADALORA\",\n",
    "        )\n",
    "        print(list(model_args.lora_target_modules.split(\",\")))\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        #model = AdaLoraModel(model, lora_config, \"default\")\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "#tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "###################################################6-mer NT###########################################################\n",
    "##############################################################################################################\n",
    "if tokenizer_name == \"bpe\":\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "             #model_args.model_name_or_path,\n",
    "            \"songlab/gpn-brassicales\",\n",
    "             model_max_length=512,\n",
    "             padding_side=\"right\",\n",
    "             use_fast=True,\n",
    "             trust_remote_code=True,\n",
    "         )\n",
    "\n",
    "    tokenizer.eos_token = tokenizer.pad_token\n",
    "    print(\"Using bpe tokenizer\")\n",
    "elif tokenizer_name == \"6-mer\":\n",
    "    tokenizer = OverlappingTokenizer(kmer_file_path)\n",
    "    print(\"Using 6-mer tokenizer\")\n",
    "elif tokenizer_name ==\"one-hot\":\n",
    "    tokenizer = OneHotTokenizer(one_hot_file_path)\n",
    "    print(\"Using one-hot tokenizer\")\n",
    "else:\n",
    "    print(\"wrong value\")\n",
    "#tokenizer = OverlappingTokenizer('/common/zhangz2lab/zhanh/dnabert-config/bert-config-6/vocab.txt')\n",
    "##############################################################################################################\n",
    "####################################################6-mer NT##########################################################\n",
    "###################################################6-mer NT###########################################################\n",
    "##############################################################################################################    \n",
    "train_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'train.csv'))\n",
    "val_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'val.csv'))\n",
    "test_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'test.csv'))\n",
    "\n",
    "#train_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'test_80.csv'))\n",
    "#val_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'val.csv'))\n",
    "#test_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'test_10.csv'))\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "####################################################6-mer NT##########################################################\n",
    "sequence = \"AAAATAAAAAGAAAAA\"\n",
    "token_ids = tokenizer(sequence)\n",
    "print(token_ids)\n",
    "sequence = \"AAAATAAATAGAAAAA\"\n",
    "token_ids = tokenizer(sequence)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0be16700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics for evaluation\n",
    "loss_name =\"contrastive+BCE\"\n",
    "def compute_metrics(eval_pred):\n",
    "    if loss_name ==\"PLLR\":\n",
    "        PLLR, labels = eval_pred\n",
    "        auc = roc_auc_score(labels, PLLR)\n",
    "        aupr = average_precision_score(labels, PLLR)\n",
    "        return {\n",
    "            'auc': auc,\n",
    "            'aupr':aupr\n",
    "        }\n",
    "    elif loss_name ==\"cosine\":\n",
    "        logits, labels = eval_pred\n",
    "#         predictions = (logits > 0.7).astype(np.int32)\n",
    "#         probabilities = (logits + 1) / 2\n",
    "        threshold = 0.97  # Example threshold, needs tuning\n",
    "\n",
    "        # Assuming logits is your cosine_sim\n",
    "        predictions = (logits < threshold).astype(np.int32)  # Dissimilar if below threshold\n",
    "        probabilities = 1 - ((logits + 1) / 2)  # Invert the mapping\n",
    "\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions)\n",
    "        recall = recall_score(labels, predictions)\n",
    "        auc = roc_auc_score(labels, probabilities)\n",
    "\n",
    "\n",
    "        # Plotting the ROC curve\n",
    "        #plt.figure()\n",
    "        #plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_curve_auc)\n",
    "        #plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        #plt.xlabel('False Positive Rate')\n",
    "        #plt.ylabel('True Positive Rate')\n",
    "        #plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        #plt.legend(loc='lower right')\n",
    "        #plt.show()\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    elif loss_name == \"contrastive\":\n",
    "        distances, labels = eval_pred\n",
    "        distances = torch.tensor(distances)\n",
    "        # Convert distances to probabilities. The larger the distance, the higher the probability of being dissimilar.\n",
    "        probabilities = torch.sigmoid(distances).numpy()  # Larger distances have higher probabilities\n",
    "        # Decide on a threshold for predictions\n",
    "        threshold = 0.3\n",
    "        predictions = (probabilities > threshold).astype(np.int32)\n",
    "\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions)\n",
    "        recall = recall_score(labels, predictions)\n",
    "        auc = roc_auc_score(labels, probabilities)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    else:\n",
    "        logits_tuple, labels = eval_pred\n",
    "        \n",
    "        print(f\"Type of logits: {type(logits_tuple)}\")\n",
    "        print(f\"Logits content (first few entries): {logits_tuple[:5]}\")  # Adjust as needed to avoid large outputs\n",
    "        logits = logits_tuple[0]\n",
    "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "        # Use the probability of class 1 (dissimilar) for evaluation\n",
    "        class_1_probabilities = probabilities[:, 1]\n",
    "\n",
    "        # Predictions based on the class with the higher probability\n",
    "        predictions = np.argmax(probabilities, axis=1)\n",
    "        class_1_threshold = 0.3  # Set your own threshold here\n",
    "\n",
    "        # Predictions based on custom threshold\n",
    "        predictions = (class_1_probabilities > class_1_threshold).astype(int)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions)\n",
    "        precision = precision_score(labels, predictions)\n",
    "        recall = recall_score(labels, predictions)\n",
    "        auc = roc_auc_score(labels, class_1_probabilities)\n",
    "        aupr = average_precision_score(labels, class_1_probabilities)\n",
    "        \n",
    "        # Save probabilities to a file\n",
    "        with open(\"./probabilities_clinvar.csv\", \"w\") as file:\n",
    "            file.write(\"Index,Probability_Class_0,Probability_Class_1\\n\")\n",
    "            for index, prob in enumerate(probabilities):\n",
    "                file.write(f\"{index},{prob[0]},{prob[1]}\\n\")\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'auc': auc,\n",
    "            'aupr':aupr\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# def custom_data_collator(data):\n",
    "#     # Here, we ensure that each item in `data` has the necessary keys.\n",
    "#     # If not, you can add handling or default values.\n",
    "#     input_ids1 = torch.stack([item['input_ids1'] for item in data])\n",
    "#     attention_mask1 = torch.stack([item['attention_mask1'] for item in data])\n",
    "#     input_ids2 = torch.stack([item['input_ids2'] for item in data])\n",
    "#     attention_mask2 = torch.stack([item['attention_mask2'] for item in data])\n",
    "\n",
    "#     # Ensure labels exist or handle its absence\n",
    "#     #labels = [item.get('labels', torch.tensor(-1)) for item in data]  # Using -1 as a default\n",
    "#     #labels = torch.stack(labels)\n",
    "#     labels = torch.stack([item['labels'] for item in data])\n",
    "\n",
    "#     return {\n",
    "#         'input_ids1': input_ids1,\n",
    "#         'attention_mask1': attention_mask1,\n",
    "#         'input_ids2': input_ids2,\n",
    "#         'attention_mask2': attention_mask2,\n",
    "#         'labels': labels\n",
    "#     }\n",
    "\n",
    "\n",
    "class GradientInspectionCallback(TrainerCallback):\n",
    "\n",
    "    def __init__(self, output_dir):\n",
    "        super(GradientInspectionCallback, self).__init__()\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)  # create directory if it doesn't exist\n",
    "\n",
    "    def on_backward_end(self, args, state, control, **kwargs):\n",
    "        print(\"Checking gradients...\")\n",
    "        model = kwargs[\"model\"]\n",
    "        \n",
    "        gradients = {}\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                gradients[name] = param.grad.tolist()  # converting tensor to list\n",
    "        \n",
    "        # Save the gradients to a file\n",
    "        file_name = os.path.join(self.output_dir, f\"gradients_step_{state.global_step}.txt\")\n",
    "        with open(file_name, 'w') as f:\n",
    "            for name, grad in gradients.items():\n",
    "                f.write(name + \":\\n\")\n",
    "                f.write(str(grad) + \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataCollator(object):\n",
    "    \"\"\"\n",
    "    Custom data collator to handle two input sequences and their respective attention masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, pad_token_id: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # Check for necessary keys in the instances\n",
    "        for key in (\"input_ids1\", \"input_ids2\", \"attention_mask1\", \"attention_mask2\", \"labels\"):\n",
    "            if not all(key in instance for instance in instances):\n",
    "                raise ValueError(f\"One or more instances does not contain the key {key}\")\n",
    "\n",
    "        # Extract the respective fields\n",
    "        input_ids1 = [item['input_ids1'] for item in instances]\n",
    "        input_ids2 = [item['input_ids2'] for item in instances]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Pad the sequences\n",
    "        input_ids1 = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids1, batch_first=True, padding_value=self.pad_token_id\n",
    "        )\n",
    "        input_ids2 = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids2, batch_first=True, padding_value=self.pad_token_id\n",
    "        )\n",
    "        \n",
    "        #input_ids1 = torch.stack([item['input_ids1'] for item in instances])\n",
    "        #input_ids2 = torch.stack([item['input_ids2'] for item in instances])\n",
    "        attention_mask1 = torch.stack([item['attention_mask1'] for item in instances])\n",
    "        attention_mask2 = torch.stack([item['attention_mask2'] for item in instances])\n",
    "        \n",
    "\n",
    "    \n",
    "        labels = torch.stack([item['labels'] for item in instances])\n",
    "\n",
    "        return {\n",
    "            'input_ids1': input_ids1,\n",
    "            'attention_mask1': attention_mask1,\n",
    "            'input_ids2': input_ids2,\n",
    "            'attention_mask2': attention_mask2,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    # Check for necessary keys in the instances\n",
    "        for key in (\"input_ids1\", \"input_ids2\", \"attention_mask1\", \"attention_mask2\", \"labels\"):\n",
    "            if not all(key in instance for instance in instances):\n",
    "                raise ValueError(f\"One or more instances does not contain the key {key}\")\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
    "        input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
    "        attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
    "        attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
    "        labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n",
    "\n",
    "        # Pad the sequences and attention masks\n",
    "        input_ids1 = torch.nn.utils.rnn.pad_sequence(input_ids1, batch_first=True, padding_value=self.pad_token_id)\n",
    "        input_ids2 = torch.nn.utils.rnn.pad_sequence(input_ids2, batch_first=True, padding_value=self.pad_token_id)\n",
    "        attention_mask1 = torch.nn.utils.rnn.pad_sequence(attention_mask1, batch_first=True, padding_value=0)\n",
    "        attention_mask2 = torch.nn.utils.rnn.pad_sequence(attention_mask2, batch_first=True, padding_value=0)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return {\n",
    "            'input_ids1': input_ids1,\n",
    "            'attention_mask1': attention_mask1,\n",
    "            'input_ids2': input_ids2,\n",
    "            'attention_mask2': attention_mask2,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "\n",
    "#custom_callback_instance = CustomDataCollator(tokenizer, pad_token_id=0)\n",
    "\n",
    "  \n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=CustomDataCollator(tokenizer, pad_token_id=0)\n",
    ")\n",
    "\n",
    "\n",
    "test_set_callback = TestSetCallback(trainer=trainer, model=model, test_dataset=test_dataset, eval_steps=training_args.eval_steps, tokenizer=tokenizer)\n",
    "trainer.add_callback(test_set_callback)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=100)  # Adjust the patience as needed\n",
    "trainer.add_callback(early_stopping_callback)\n",
    "\n",
    "#gradient_inspection_callback = GradientInspectionCallback(output_dir = training_args.output_dir)\n",
    "#trainer.add_callback(gradient_inspection_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31f07743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTATTTGCTGGTTAATTCCTTGGTTTAATTTCCTCTTTTAGGGTGAAATTGGAGCTGTTGGTAACGCTGGTCCTGCTGGTCCCGCCGGTCCCCGTGGTGAAGTGGGTCTTCCAGGCCTCTCCGGCCCCGTTGGACCTCCTGTAAGTAGCCACTGTCTTTAAACTTTATTG\n",
      "['t', 't', 'a', 't', 't', 't', 'g', 'c', 't', 'g', 'g', 't', 't', 'a', 'a', 't', 't', 'c', 'c', 't', 't', 'g', 'g', 't', 't', 't', 'a', 'a', 't', 't', 't', 'c', 'c', 't', 'c', 't', 't', 't', 't', 'a', 'g', 'g', 'g', 't', 'g', 'a', 'a', 'a', 't', 't', 'g', 'g', 'a', 'g', 'c', 't', 'g', 't', 't', 'g', 'g', 't', 'a', 'a', 'c', 'g', 'c', 't', 'g', 'g', 't', 'c', 'c', 't', 'g', 'c', 't', 'g', 'g', 't', 'c', 'c', 'c', 'g', 'c', 'c', 'g', 'g', 't', 'c', 'c', 'c', 'c', 'g', 't', 'g', 'g', 't', 'g', 'a', 'a', 'g', 't', 'g', 'g', 'g', 't', 'c', 't', 't', 'c', 'c', 'a', 'g', 'g', 'c', 'c', 't', 'c', 't', 'c', 'c', 'g', 'g', 'c', 'c', 'c', 'c', 'g', 't', 't', 'g', 'g', 'a', 'c', 'c', 't', 'c', 'c', 't', 'g', 't', 'a', 'a', 'g', 't', 'a', 'g', 'c', 'c', 'a', 'c', 't', 'g', 't', 'c', 't', 't', 't', 'a', 'a', 'a', 'c', 't', 't', 't', 'a', 't', 't', 'g']\n",
      "170\n",
      "torch.Size([16, 170])\n",
      "torch.Size([16, 170])\n",
      "torch.Size([16, 170])\n",
      "torch.Size([16, 170])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "with open(os.path.join(data_path, 'train.csv'), 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)  # skip the header\n",
    "    first_row = next(reader)  # reads the first row after the header\n",
    "    print(first_row[0]) \n",
    "\n",
    "sample_sequence = first_row[0]\n",
    "tokenized_sample = tokenizer.tokenize(sample_sequence)\n",
    "print(tokenized_sample)\n",
    "print(len(tokenized_sample))\n",
    "# Initialize datasets and collator\n",
    "##############################################################################################################\n",
    "####################################################6-mer NT##########################################################\n",
    "\n",
    "train_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'train.csv'))\n",
    "collator = CustomDataCollator(tokenizer, pad_token_id=0)\n",
    "\n",
    "##############################################################################################################\n",
    "####################################################6-mer NT##########################################################\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator)\n",
    "\n",
    "# Fetch and visualize one batch\n",
    "for batch in train_loader:\n",
    "    print(batch[\"input_ids1\"].shape)  # Should be [16, seq_length] \n",
    "    print(batch[\"input_ids2\"].shape)  # Should be [16, seq_length]\n",
    "    print(batch[\"attention_mask1\"].shape)  # Should be [16, seq_length]\n",
    "    print(batch[\"attention_mask2\"].shape)  # Should be [16, seq_length]\n",
    "    print(batch[\"labels\"].shape)  # Should be [16]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9ff78a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124.45924377441406 11.148496627807617\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Given your previously defined Dataset and DataLoader:\n",
    "\n",
    "##############################################################################################################\n",
    "####################################################6-mer NT##########################################################\n",
    "train_dataset = SiameseDataset(tokenizer, tokenizer_name, os.path.join(data_path, 'train.csv'))\n",
    "##############################################################################################################\n",
    "####################################################6-mer NT##########################################################\n",
    "# Get the first training sample:\n",
    "first_sample = train_dataset[0]\n",
    "\n",
    "# Define your Siamese Network model:\n",
    "#model = SiameseNetwork('bert-base-uncased', 2)\n",
    "\n",
    "# Perform a forward pass using the first sample:\n",
    "with torch.no_grad():\n",
    "    loss, logits, cosine_sim = model(\n",
    "        first_sample[\"input_ids1\"].unsqueeze(0).to(device),  # Add batch dimension\n",
    "        first_sample[\"attention_mask1\"].unsqueeze(0).to(device),  # Add batch dimension\n",
    "        first_sample[\"input_ids2\"].unsqueeze(0).to(device),  # Add batch dimension\n",
    "        first_sample[\"attention_mask2\"].unsqueeze(0).to(device),  # Add batch dimension\n",
    "        first_sample[\"labels\"].unsqueeze(0).to(device)  # Add batch dimension\n",
    "    )\n",
    "\n",
    "print(loss.item(), cosine_sim.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "718f3f78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='694' max='347' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [347/347 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.6538396 , -0.11485106],\n",
      "       [ 0.15583542,  0.9051387 ],\n",
      "       [-0.05200773,  0.88027865],\n",
      "       ...,\n",
      "       [ 0.81824803,  0.33438763],\n",
      "       [-0.25327438, -2.8477368 ],\n",
      "       [-0.98532575,  2.5087268 ]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n",
      "{'eval_loss': 233.39637756347656, 'eval_accuracy': 0.4823359769286229, 'eval_f1': 0.04774535809018568, 'eval_precision': 0.025423728813559324, 'eval_recall': 0.391304347826087, 'eval_auc': 0.41513066173848195, 'eval_aupr': 0.02922271541760056, 'eval_runtime': 5.5701, 'eval_samples_per_second': 498.017, 'eval_steps_per_second': 62.297}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13870' max='13870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13870/13870 05:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Aupr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>220.778200</td>\n",
       "      <td>233.063263</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511417</td>\n",
       "      <td>0.038761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>231.017200</td>\n",
       "      <td>233.062881</td>\n",
       "      <td>0.967195</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.558060</td>\n",
       "      <td>0.054407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>228.958400</td>\n",
       "      <td>233.062698</td>\n",
       "      <td>0.967195</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.578040</td>\n",
       "      <td>0.103680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>223.846100</td>\n",
       "      <td>233.062683</td>\n",
       "      <td>0.965032</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.662496</td>\n",
       "      <td>0.094091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>226.481100</td>\n",
       "      <td>233.062592</td>\n",
       "      <td>0.963951</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.655070</td>\n",
       "      <td>0.101336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>225.213800</td>\n",
       "      <td>233.062500</td>\n",
       "      <td>0.966474</td>\n",
       "      <td>0.041237</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.651465</td>\n",
       "      <td>0.120479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>234.521200</td>\n",
       "      <td>233.062271</td>\n",
       "      <td>0.967556</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.660855</td>\n",
       "      <td>0.154314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>221.446200</td>\n",
       "      <td>233.062302</td>\n",
       "      <td>0.967195</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.654103</td>\n",
       "      <td>0.134972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>228.163800</td>\n",
       "      <td>233.062225</td>\n",
       "      <td>0.967195</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.675396</td>\n",
       "      <td>0.149293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>226.015700</td>\n",
       "      <td>233.062180</td>\n",
       "      <td>0.967195</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.684669</td>\n",
       "      <td>0.142337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>224.490300</td>\n",
       "      <td>233.062180</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.703267</td>\n",
       "      <td>0.155879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>230.542200</td>\n",
       "      <td>233.062088</td>\n",
       "      <td>0.967556</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.695539</td>\n",
       "      <td>0.160395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>224.000500</td>\n",
       "      <td>233.062119</td>\n",
       "      <td>0.967556</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.706939</td>\n",
       "      <td>0.160316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 3.7853537 , -1.2482939 ],\n",
      "       [ 2.8166833 , -0.8757632 ],\n",
      "       [ 2.7930155 , -1.3994352 ],\n",
      "       ...,\n",
      "       [ 3.48699   , -1.2170808 ],\n",
      "       [ 0.37386596, -3.6639984 ],\n",
      "       [ 2.181161  ,  0.6971541 ]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/zhanh/anaconda3/envs/dna_gpn/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Lambda: 0.05\n",
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.848447  , -0.05335187],\n",
      "       [ 3.1199257 , -1.2569449 ],\n",
      "       [ 2.836166  , -1.4793344 ],\n",
      "       ...,\n",
      "       [ 2.9842422 , -0.8695714 ],\n",
      "       [ 0.51105714, -3.489455  ],\n",
      "       [ 2.0178816 ,  0.6941728 ]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 3.3171482 , -0.3686895 ],\n",
      "       [ 2.8421776 , -1.1405561 ],\n",
      "       [ 2.3948636 , -1.159321  ],\n",
      "       ...,\n",
      "       [ 3.3788414 , -1.2346762 ],\n",
      "       [ 0.74977577, -3.5270336 ],\n",
      "       [ 2.1168902 ,  0.61133534]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, Lambda: 0.05\n",
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.5485022 ,  0.16800776],\n",
      "       [ 3.181114  , -1.3054383 ],\n",
      "       [ 2.5657492 , -1.2809474 ],\n",
      "       ...,\n",
      "       [ 2.9509246 , -1.0374624 ],\n",
      "       [-0.20370801, -2.768267  ],\n",
      "       [ 2.4779232 ,  0.3930683 ]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.9429893 , -0.42901102],\n",
      "       [ 2.9934072 , -1.2922808 ],\n",
      "       [ 2.6670346 , -1.5946938 ],\n",
      "       ...,\n",
      "       [ 2.9621835 , -0.92571867],\n",
      "       [-0.01124464, -3.0182693 ],\n",
      "       [ 2.1784668 ,  0.48854506]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000, Lambda: 0.05\n",
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.4863517 , -0.16040128],\n",
      "       [ 2.479605  , -1.3354403 ],\n",
      "       [ 2.241522  , -1.2993957 ],\n",
      "       ...,\n",
      "       [ 2.5996668 , -1.273456  ],\n",
      "       [ 0.14964889, -3.078916  ],\n",
      "       [ 1.7908126 ,  0.38876206]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.5517187 , -0.5087202 ],\n",
      "       [ 2.4337983 , -1.4899684 ],\n",
      "       [ 2.146466  , -1.529249  ],\n",
      "       ...,\n",
      "       [ 2.652978  , -1.2880787 ],\n",
      "       [ 0.11406742, -3.0845237 ],\n",
      "       [ 1.7790608 ,  0.22176804]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000, Lambda: 0.05\n",
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.632395  , -0.5364266 ],\n",
      "       [ 2.8125727 , -1.5260321 ],\n",
      "       [ 2.156622  , -1.4537103 ],\n",
      "       ...,\n",
      "       [ 2.5693386 , -1.2238612 ],\n",
      "       [ 0.1144962 , -2.8445365 ],\n",
      "       [ 2.028234  ,  0.25584784]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.3195875 , -0.37318313],\n",
      "       [ 2.8316941 , -1.5310118 ],\n",
      "       [ 2.034794  , -1.4493454 ],\n",
      "       ...,\n",
      "       [ 2.735299  , -1.3513541 ],\n",
      "       [ 0.14710692, -2.7334485 ],\n",
      "       [ 1.8730507 ,  0.3483761 ]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000, Lambda: 0.05\n",
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.4846492 , -0.53461415],\n",
      "       [ 2.7312083 , -1.5740203 ],\n",
      "       [ 2.4672675 , -1.7740263 ],\n",
      "       ...,\n",
      "       [ 2.7805057 , -1.4246665 ],\n",
      "       [ 0.30228248, -3.037864  ],\n",
      "       [ 1.8469949 ,  0.28236532]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.6140378 , -0.7091942 ],\n",
      "       [ 2.979992  , -1.7781773 ],\n",
      "       [ 2.5284572 , -1.8121697 ],\n",
      "       ...,\n",
      "       [ 3.0175662 , -1.6036938 ],\n",
      "       [ 0.33741316, -3.0462217 ],\n",
      "       [ 1.9981341 ,  0.16397718]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12000, Lambda: 0.05\n",
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.6089754 , -0.7536131 ],\n",
      "       [ 2.6858032 , -1.6816589 ],\n",
      "       [ 2.2790515 , -1.6632493 ],\n",
      "       ...,\n",
      "       [ 2.8684134 , -1.5301031 ],\n",
      "       [ 0.00460648, -2.8813775 ],\n",
      "       [ 1.799075  ,  0.23532239]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of logits: <class 'tuple'>\n",
      "Logits content (first few entries): (array([[ 2.3806846 , -0.47809276],\n",
      "       [ 2.693931  , -1.650042  ],\n",
      "       [ 2.224239  , -1.6351177 ],\n",
      "       ...,\n",
      "       [ 2.801206  , -1.5154084 ],\n",
      "       [-0.00615284, -2.8338747 ],\n",
      "       [ 1.9170696 ,  0.18560371]], dtype=float32), array([10.742352 , 16.392246 , 11.518335 , ...,  8.764733 ,  6.6363187,\n",
      "       18.599775 ], dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710379/374013838.py:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids1 = [torch.tensor(item['input_ids1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:214: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids2 = [torch.tensor(item['input_ids2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:215: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask1 = [torch.tensor(item['attention_mask1'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:216: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask2 = [torch.tensor(item['attention_mask2'], dtype=torch.long) for item in instances]\n",
      "/tmp/ipykernel_710379/374013838.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = [torch.tensor(item['labels'], dtype=torch.long) for item in instances]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13870, training_loss=226.5175951615975, metrics={'train_runtime': 357.8736, 'train_samples_per_second': 309.97, 'train_steps_per_second': 38.757, 'total_flos': 0.0, 'train_loss': 226.5175951615975, 'epoch': 5.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "#results = trainer.evaluate()\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35688c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNA_GPN_Kernel",
   "language": "python",
   "name": "dna_gpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
